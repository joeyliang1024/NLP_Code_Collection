{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep  8 17:53:49 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 12.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:1B:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    44W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m optim\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/__init__.py:1263\u001b[0m\n\u001b[1;32m   1261\u001b[0m \u001b[39m# nn.quant* depends on ao -- so should be after those.\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantizable\u001b[39;00m\n\u001b[0;32m-> 1263\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantized\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mqat\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mintrinsic\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/quantized/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m dynamic  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m functional  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m modules  \u001b[39m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/quantized/dynamic/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mao\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantized\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdynamic\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/ao/nn/quantized/dynamic/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/ao/nn/quantized/dynamic/modules/__init__.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlinear\u001b[39;00m \u001b[39mimport\u001b[39;00m Linear\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mrnn\u001b[39;00m \u001b[39mimport\u001b[39;00m LSTM, GRU, LSTMCell, RNNCell, GRUCell\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconv\u001b[39;00m \u001b[39mimport\u001b[39;00m Conv1d, Conv2d, Conv3d, ConvTranspose1d, ConvTranspose2d, ConvTranspose3d\n\u001b[1;32m      6\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mLinear\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mLSTM\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mConvTranspose3d\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m ]\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1149\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1069\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:729\u001b[0m, in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import regex\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "hidden_size = 512\n",
    "batch_size  = 80\n",
    "epochs      = 100\n",
    "MAX_LENGTH = 200\n",
    "# load\n",
    "LOAD_MODEL = False\n",
    "SOURCE_LANGUAGE = \"中文\"\n",
    "TARGET_LANGUAGE = \"台文\"\n",
    "WITHOUT_聖經 = True\n",
    "if WITHOUT_聖經:\n",
    "    model_path = f\"model/Scratch_model/{SOURCE_LANGUAGE}翻{TARGET_LANGUAGE}_無聖經_Embed{hidden_size}_batch{batch_size}_epoch{epochs}.pt\"\n",
    "else:\n",
    "    model_path = f\"model/Scratch_model/{SOURCE_LANGUAGE}翻{TARGET_LANGUAGE}_有聖經_Embed{hidden_size}_batch{batch_size}_epoch{epochs}.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of traning data: 16873\n"
     ]
    }
   ],
   "source": [
    "# %pip install KeSi\n",
    "from kesi import Ku\n",
    "def lomaji2POJ(lomaji)->str:\n",
    "    '''\n",
    "    轉白話字\n",
    "    '''\n",
    "    ji_ls = re.split(r' |\\xa0|\\u3000', str(lomaji))\n",
    "    trans_ji = ' '.join([Ku(ji).POJ().hanlo for ji in ji_ls])\n",
    "    return trans_ji\n",
    "def lomaji2KIP(lomaji)->str:\n",
    "    '''\n",
    "    轉羅馬字\n",
    "    '''\n",
    "    ji_ls = re.split(r' |\\xa0|\\u3000', str(lomaji))\n",
    "    trans_ji = ' '.join([Ku(ji).KIP().hanlo for ji in ji_ls])\n",
    "    return trans_ji\n",
    "\n",
    "def fill_na(row):\n",
    "    if pd.isnull(row['台羅']):\n",
    "        row['台羅'] = lomaji2KIP(row['白話字'])\n",
    "    if pd.isnull(row['白話字']):\n",
    "        row['白話字'] = lomaji2POJ(row['台羅'])\n",
    "    return row\n",
    "\n",
    "# load data\n",
    "df1 = pd.read_csv(\"data/moedict_平行_人工調整.csv\")\n",
    "df2 = pd.read_csv(\"../平行語料/聖經平行語料_p_final.csv\")\n",
    "df3 = pd.read_csv(\"../平行語料/TAT_p.csv\")\n",
    "if WITHOUT_聖經:\n",
    "    df = pd.concat([df1, df3], axis=0).drop_duplicates()\n",
    "else:\n",
    "    df = pd.concat([df1, df2, df3], axis=0).drop_duplicates()\n",
    "df = df[['中文', '台文', '台羅', '白話字']]\n",
    "# 轉白話字或漢羅\n",
    "df = df.apply(fill_na, axis=1).dropna()\n",
    "df = df[df[SOURCE_LANGUAGE].str.len() <= MAX_LENGTH]\n",
    "df = df[df[TARGET_LANGUAGE].str.len() <= MAX_LENGTH]\n",
    "print(f\"Number of traning data: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "UNK_token = 3\n",
    "\n",
    "def split_POJ_sentence(sentence):\n",
    "    pattern = r'(?=-)|(?<=\\p{Punct})|(?=\\p{Punct})|(?<=\\s)|(?=\\s)'\n",
    "    final_result = []\n",
    "    result = regex.split(pattern, sentence)\n",
    "    for i, word in enumerate(result):\n",
    "        if i > 0 and result[i-1] == '-':\n",
    "            final_result[-1] += word\n",
    "        else:\n",
    "            final_result.append(word)\n",
    "    return [word for word in final_result if word]\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"[PAD]\": 0,\n",
    "                           \"[SOS]\": 1, \n",
    "                           \"[EOS]\": 2, \n",
    "                           \"[UNK]\": 3}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"[PAD]\", \n",
    "                           1: \"[SOS]\", \n",
    "                           2: \"[EOS]\", \n",
    "                           3: \"[UNK]\"}\n",
    "        self.n_words = 4  # Count SOS and EOS\n",
    "\n",
    "    def add_Sentence_Word(self, sentence):\n",
    "        if self.name == \"中文\" or self.name == \"台文\":\n",
    "            for _ in sentence:\n",
    "                self.addWord(_)\n",
    "        else:\n",
    "            split_sen = split_POJ_sentence(sentence)\n",
    "            for _ in split_sen:\n",
    "                self.addWord(_)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def add_Vocab(self, vocab_path):\n",
    "        with open(vocab_path, 'r', encoding='utf-8') as file:\n",
    "            vocab_list = [vocab.rstrip() for vocab in file.readlines()]\n",
    "        for _ in vocab_list[5:]:\n",
    "            if _ not in self.word2index:\n",
    "                self.word2index[_] = self.n_words\n",
    "                self.word2count[_] = 1\n",
    "                self.index2word[self.n_words] = _\n",
    "                self.n_words += 1\n",
    "            else:\n",
    "                self.word2count[_] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DataFrame...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 16873 sentence pairs\n",
      "Adding vocab or words...\n",
      "Counted words:\n",
      "中文 3414\n",
      "台文 3407\n",
      "Input: 中文\n",
      "Output: 台文\n",
      "('拜託他做事，他都做得很用心。', '拜託伊做代誌，伊攏做甲真入心。')\n"
     ]
    }
   ],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def load_Langs(lang1, lang2, df, reverse=False):\n",
    "    print(\"Reading DataFrame...\")\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = []\n",
    "    for _ in range(df.shape[0]):\n",
    "        if lang1 == \"中文\" or lang1 == \"台文\":\n",
    "            #pair1 = unicodeToAscii(df[lang1].iloc[_])\n",
    "            pair1 = df[lang1].iloc[_]\n",
    "        else: \n",
    "            pair1 = normalizeString(df[lang1].iloc[_])\n",
    "        if lang2 == \"中文\" or lang2 == \"台文\":\n",
    "            #pair2 = unicodeToAscii(df[lang2].iloc[_])\n",
    "            pair2 = df[lang2].iloc[_]\n",
    "        else: \n",
    "            pair2 = normalizeString(df[lang2].iloc[_])\n",
    "        pairs.append((pair1, pair2))\n",
    "    # Reverse pairs, make Lang instances\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "def prepareData(lang1, lang2, df, reverse=False):\n",
    "    input_lang, output_lang, pairs = load_Langs(lang1, lang2, df, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Adding vocab or words...\")\n",
    "    # add vocab\n",
    "    for pair in pairs:\n",
    "        input_lang.add_Sentence_Word(pair[0])\n",
    "        output_lang.add_Sentence_Word(pair[1])\n",
    "\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# sample\n",
    "input_lang, output_lang, pairs = prepareData(SOURCE_LANGUAGE, TARGET_LANGUAGE, df, False)\n",
    "print(f\"Input: {input_lang.name}\\nOutput: {output_lang.name}\")\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_translator(nn.Module):\n",
    "    def __init__(self, hidden_size, input_lang, output_lang):\n",
    "        super(transformer_translator, self).__init__()\n",
    "        self.encoder = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "        self.decoder = AttnDecoderRNN(hidden_size, output_lang.n_words)\n",
    "    def forward(self, input_tensor, target_tensor = None):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = self.decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Preprocess (Define and Prepare Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DataFrame...\n",
      "Read 16873 sentence pairs\n",
      "Adding vocab or words...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "中文 3414\n",
      "台文 3407\n",
      "Input: 中文\n",
      "Output: 台文\n",
      "('煙往上衝', '煙蓬蓬衝')\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 31.75 GiB total capacity; 5.64 GiB already allocated; 18.75 MiB free; 5.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     train_dataloader \u001b[39m=\u001b[39m DataLoader(train_data, sampler\u001b[39m=\u001b[39mtrain_sampler, batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m input_lang, output_lang, train_dataloader\n\u001b[0;32m---> 63\u001b[0m input_lang, output_lang, train_dataloader \u001b[39m=\u001b[39m get_dataloader(batch_size)\n",
      "Cell \u001b[0;32mIn[30], line 57\u001b[0m, in \u001b[0;36mget_dataloader\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     53\u001b[0m     input_ids[idx, :\u001b[39mlen\u001b[39m(inp_ids)] \u001b[39m=\u001b[39m inp_ids\n\u001b[1;32m     54\u001b[0m     target_ids[idx, :\u001b[39mlen\u001b[39m(tgt_ids)] \u001b[39m=\u001b[39m tgt_ids\n\u001b[1;32m     56\u001b[0m train_data \u001b[39m=\u001b[39m TensorDataset(torch\u001b[39m.\u001b[39mLongTensor(input_ids)\u001b[39m.\u001b[39mto(device),\n\u001b[0;32m---> 57\u001b[0m                            torch\u001b[39m.\u001b[39mLongTensor(target_ids)\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m     59\u001b[0m train_sampler \u001b[39m=\u001b[39m RandomSampler(train_data)\n\u001b[1;32m     60\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(train_data, sampler\u001b[39m=\u001b[39mtrain_sampler, batch_size\u001b[39m=\u001b[39mbatch_size)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 31.75 GiB total capacity; 5.64 GiB already allocated; 18.75 MiB free; 5.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    '''\n",
    "    \"我是一個句子\" --> [0,1,2,3,4,5]\n",
    "    '''\n",
    "    if lang.name ==\"中文\" or lang.name ==\"台文\":\n",
    "        encode= []\n",
    "        for word in sentence:\n",
    "            if word in lang.word2index.keys():\n",
    "                encode.append(lang.word2index[word])\n",
    "            else:\n",
    "                encode.append(lang.word2index[\"[UNK]\"])\n",
    "        return encode\n",
    "    else:\n",
    "        encode= []\n",
    "        for word in split_POJ_sentence(sentence):\n",
    "            if word in lang.word2index.keys():\n",
    "                encode.append(lang.word2index[word])\n",
    "            else:\n",
    "                encode.append(lang.word2index[\"[UNK]\"])\n",
    "        return encode\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    '''\n",
    "    \"我是一個句子\" --> tensor([0,1,2,3,4,5])\n",
    "    '''\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    '''\n",
    "    (\"我是一個句子\", \"第二句\") --> (tensor([0,1,2,3,4,5]), tensor([6,7,4]))\n",
    "    '''\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData(SOURCE_LANGUAGE, TARGET_LANGUAGE, df, False)\n",
    "    print(f\"Input: {input_lang.name}\\nOutput: {output_lang.name}\")\n",
    "    print(random.choice(pairs))\n",
    "    \n",
    "    n = len(pairs)\n",
    "    # [PAD] here\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  99,   32, 1721, 1722,  685,  101, 2977,  175,   41,    2,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_tensor, target_tensor = data\n",
    "        decoder_outputs = model(input_tensor, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_Sentence(model, sentence, input_lang, output_lang):\n",
    "    # beam search\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        decoder_outputs = model(input_tensor)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('') # \"EOS\"\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, model, n_epochs, learning_rate=0.001, \n",
    "          print_every=100, plot_every=100):\n",
    "    print(\"Start training:   \")\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total  = 0  # Reset every plot_every\n",
    "    best_loss = float(\"inf\")\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_token) # ignore PAD\n",
    "    test_pair = random.choice(pairs)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        print(f'Epoch: {epoch}')\n",
    "        loss = train_epoch(train_dataloader, model, optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        print(f'loss: {loss}, best loss: {best_loss}')\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            # save model\n",
    "            torch.save(model.state_dict(), model_path) #\"台翻中\"\n",
    "            print(f\"model is save at\\n{model_path}\")\n",
    "            \n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "            print(\"Testing a translation sample: \")\n",
    "            print(f'Source Sentence ({SOURCE_LANGUAGE:3s}): ', test_pair[0])\n",
    "            print(f'Target Sentence ({TARGET_LANGUAGE:3s}): ', test_pair[1])\n",
    "            output_words = evaluate_Sentence(model, test_pair[0], input_lang, output_lang)\n",
    "            print('Translation Sentence:   ', ''.join(output_words))\n",
    "            \n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(model, n=5):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('Source:\\n', pair[0])\n",
    "        print('Target:\\n', pair[1])\n",
    "        output_words = evaluate_Sentence(model, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ''.join(output_words)\n",
    "        print('Translation:\\n', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer_translator(hidden_size, input_lang, output_lang).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train model with parameters: embedding dimension:512 batch size:70 epoches:100\n",
      "Start training:   \n",
      "Epoch: 1\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 31.75 GiB total capacity; 5.64 GiB already allocated; 18.75 MiB free; 5.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain model with parameters: embedding dimension:\u001b[39m\u001b[39m{\u001b[39;00mhidden_size\u001b[39m}\u001b[39;00m\u001b[39m batch size:\u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39m epoches:\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     train(train_dataloader, model, epochs, print_every\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, plot_every\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, model, n_epochs, learning_rate, print_every, plot_every)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     14\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     loss \u001b[39m=\u001b[39m train_epoch(train_dataloader, model, optimizer, criterion)\n\u001b[1;32m     16\u001b[0m     print_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m     17\u001b[0m     plot_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(dataloader, model, optimizer, criterion)\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      6\u001b[0m input_tensor, target_tensor \u001b[39m=\u001b[39m data\n\u001b[0;32m----> 7\u001b[0m decoder_outputs \u001b[39m=\u001b[39m model(input_tensor, target_tensor)\n\u001b[1;32m      9\u001b[0m loss \u001b[39m=\u001b[39m criterion(\n\u001b[1;32m     10\u001b[0m     decoder_outputs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, decoder_outputs\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)),\n\u001b[1;32m     11\u001b[0m     target_tensor\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m, in \u001b[0;36mtransformer_translator.forward\u001b[0;34m(self, input_tensor, target_tensor)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_tensor, target_tensor \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m      7\u001b[0m     encoder_outputs, encoder_hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(input_tensor)\n\u001b[0;32m----> 8\u001b[0m     decoder_outputs, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(encoder_outputs, encoder_hidden, target_tensor)\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 34\u001b[0m, in \u001b[0;36mAttnDecoderRNN.forward\u001b[0;34m(self, encoder_outputs, encoder_hidden, target_tensor)\u001b[0m\n\u001b[1;32m     31\u001b[0m attentions \u001b[39m=\u001b[39m []\n\u001b[1;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(MAX_LENGTH):\n\u001b[0;32m---> 34\u001b[0m     decoder_output, decoder_hidden, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_step(\n\u001b[1;32m     35\u001b[0m         decoder_input, decoder_hidden, encoder_outputs\n\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     37\u001b[0m     decoder_outputs\u001b[39m.\u001b[39mappend(decoder_output)\n\u001b[1;32m     38\u001b[0m     attentions\u001b[39m.\u001b[39mappend(attn_weights)\n",
      "Cell \u001b[0;32mIn[8], line 59\u001b[0m, in \u001b[0;36mAttnDecoderRNN.forward_step\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     56\u001b[0m embedded \u001b[39m=\u001b[39m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(\u001b[39minput\u001b[39m))\n\u001b[1;32m     58\u001b[0m query \u001b[39m=\u001b[39m hidden\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m context, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(query, encoder_outputs)\n\u001b[1;32m     60\u001b[0m input_gru \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((embedded, context), dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     62\u001b[0m output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgru(input_gru, hidden)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m, in \u001b[0;36mBahdanauAttention.forward\u001b[0;34m(self, query, keys)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, query, keys):\n\u001b[0;32m----> 9\u001b[0m     scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mVa(torch\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWa(query) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mUa(keys)))\n\u001b[1;32m     10\u001b[0m     scores \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39msqueeze(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m     weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 31.75 GiB total capacity; 5.64 GiB already allocated; 18.75 MiB free; 5.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "if LOAD_MODEL:\n",
    "    # Load  MOdel\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    print(f\"train model with parameters: embedding dimension:{hidden_size} batch size:{batch_size} epoches:{epochs}\")\n",
    "    train(train_dataloader, model, epochs, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:\n",
      " 這是挪亞的三個兒子，他們的後裔散佈全地。\n",
      "Target:\n",
      " 此三個是挪亞的子；𪜶的後代分散去全世界。\n",
      "Translation:\n",
      " 挪亞全三分裂全地。\n",
      "\n",
      "Source:\n",
      " 這個東西被人冒名領走了。\n",
      "Target:\n",
      " 這項物件予人食名領去矣。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation:\n",
      " 彼蕊物件由矣。\n",
      "\n",
      "Source:\n",
      " 兩根柱子和柱子頂上兩個如碗的柱頂，以及蓋着如碗柱頂的兩個網子；\n",
      "Target:\n",
      " 伊所造的有：\n",
      "兩支柱、柱頂碗形的柱斗、蓋碗形柱斗的網仔；\n",
      "Translation:\n",
      " 托廟妝飾柱頂的柱斗；柱斗頂妝飾柱斗造兩綴銅碗牆頂碗牆角暈製造妝飾鍊鍊雕刻鍊𫞼兩圈妝飾圈套芳油雕造斗𫞼造兩斗柱斗邊碗牆造兩斗斗柱斗的網圈套妝飾柱斗的網仔斗造斗斗角暈幔造妝飾牙斗造妝飾牙籠造兩籠妝飾汗。\n",
      "\n",
      "Source:\n",
      " 要娶妻生兒養女，為你們的兒子娶妻，使你們的女兒嫁人，生兒養女。你們要在那裏生養眾多，不可減少。\n",
      "Target:\n",
      " 娶某生子，互恁的後生查某子結婚，養育子女。恁著佇遐生湠，毋通互恁的人口減少。\n",
      "Translation:\n",
      " 娶某子養飼女子替翁某囝作某，生子兒女。恁著佇遐生湠真多，毋通減少。\n",
      "\n",
      "Source:\n",
      " 於是耶穌說：「婦人，你的信心好大呀！照你所要的，給你成全吧！」她的女兒就在那時候好起來了。\n",
      "Target:\n",
      " 後來，耶穌應講：「婦仁人，你的信心真大。互你的心願實現！」伊的查某子佇彼陣就得著醫好。\n",
      "Translation:\n",
      " 然婦婦仁人，你的信心愈赴功效！」伊的查某子𫢶佇彼時陣。\n",
      "\n",
      "Source:\n",
      " 你看，他還公開講道，他們也不對他說甚麼。難道官長真的認為這是基督嗎？\n",
      "Target:\n",
      " 你看，伊公開咧講話，𪜶嘛無講什麼給伊反對。豈講諸個官長真正知此個人是基督？\n",
      "Translation:\n",
      " 開別句話歪喙，𪜶嘛𣍐給伊赦免什麼。\n",
      "\n",
      "Source:\n",
      " 所羅門王用黎巴嫩木，\n",
      "為自己製造車子。\n",
      "Target:\n",
      " 所羅門王用黎巴嫩的柴，\n",
      "為家己做一頂轎。\n",
      "Translation:\n",
      " 黎巴嫩林，\n",
      "掘池臟持樓梯。\n",
      "\n",
      "Source:\n",
      " 於是末底改照以斯帖一切所吩咐的去做。\n",
      "Target:\n",
      " 末底改就照以斯帖的吩咐去做。\n",
      "Translation:\n",
      " 再換以斯帖之楗\n",
      "\n",
      "Source:\n",
      " 於是，約書亞派遣他們前去。他們行軍到埋伏的地方，伏在伯特利和艾城的中間，就是艾城的西邊。這夜，約書亞在士兵中間過夜。\n",
      "Target:\n",
      " 約書亞派𪜶去，𪜶就去埋伏佇艾城的西旁，伯特利及艾城的中間。彼暝，約書亞及戰士做夥佇軍營歇睏。\n",
      "Translation:\n",
      " 約瑟派偵探敘利亞畢閃避輪；𪜶𤆬伏伏兵閃避埋伏閃避掃羅珥滅亡。這暝暝時，約書亞佇士革夫中間。\n",
      "\n",
      "Source:\n",
      " 拜託睿麒兄幫我印讀書會，\n",
      "Target:\n",
      " 拜託睿麒兄替我印讀冊會，\n",
      "Translation:\n",
      " 引誘𤆬我讀冊，\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "evaluateRandomly(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = 蔡崇名目前還是十八個認養兒，還是國內、國外各半的乾爹，是雲嘉南區最多乾兒子的愛心爸爸。\n",
      "output = 押沙龍眼副養飼脯浴料還予政府、加巾舍票車藤懸戀耍呢乾丸怪？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = 家扶中心社工和認養兒，稱呼他是「聯合國爸爸」，代表他的愛心不分海內外，更加重要是持續至今，對他有著深深感恩和尊敬。\n",
      "output = 扶賊腐心佮認養主張聯聯盟友」，代表達薛月予東方斗閣較闊蒂頭足今暝日逼耙謳咾輕鬆，閣共占深事惹天情卑輕鬆威。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_500/1848875152.py:8: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + input_sentence.split(' ') +['<EOS>'], rotation=90)\n",
      "/tmp/ipykernel_500/1848875152.py:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + output_words)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = 蔡崇名是現任新營市大宏里長，和新營公益慈善基金會董事長。\n",
      "output = 十萬新館市林口疊車踅、社營圍困 美妙直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直直徑屠高速直直。\n",
      "input = 難道要強化兩岸經貿交流，也必須透過降低台灣的民主自由程度，來達成與中國同樣極權統治的水準？\n",
      "output = 恁化滅負責任泰爾兇省，嘛著想欲降低此塔、民爭執政統治國內成做游泳池？\n"
     ]
    }
   ],
   "source": [
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words = evaluate_Sentence(model, input_sentence, input_lang, output_lang)\n",
    "    print('input  =', input_sentence)\n",
    "    print('output =', ''.join(output_words))\n",
    "\n",
    "evaluateAndShowAttention(pairs[1][0])\n",
    "evaluateAndShowAttention(pairs[2][0])\n",
    "evaluateAndShowAttention(pairs[3][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU adn CHRF++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/iCorpus_test_sample100.csv\").drop_duplicates().dropna()\n",
    "test_df = test_df[test_df[SOURCE_LANGUAGE].str.len() <= MAX_LENGTH]\n",
    "test_df = test_df[test_df[TARGET_LANGUAGE].str.len() <= MAX_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU:   0.51159\n",
      "CHRF++: 0.50190\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "def compute_batch_sentence_BLEU(translation_sentence_list:list, correct_sentence_list:list):\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    results = bleu.compute(predictions=translation_sentence_list, \n",
    "                           references=correct_sentence_list,\n",
    "                           smooth=True)\n",
    "    return results['bleu']\n",
    "\n",
    "def compute_batch_sentence_CHRF_plus_plus(translation_sentence_list:list, correct_sentence_list:list):\n",
    "    chrf = evaluate.load(\"chrf\")\n",
    "    results = chrf.compute(predictions=translation_sentence_list, \n",
    "                           references=correct_sentence_list,\n",
    "                           word_order=2)\n",
    "    return results['score']\n",
    "    \n",
    "def generate(input_sentence, for_bleu = False):\n",
    "    output_words = evaluate_Sentence(model, input_sentence, input_lang, output_lang)\n",
    "    if for_bleu:\n",
    "        output_words = \" \".join(output_words)\n",
    "    else:\n",
    "        output_words = \"\".join(output_words)\n",
    "    return output_words\n",
    "\n",
    "test_df['翻譯'+TARGET_LANGUAGE] = test_df[SOURCE_LANGUAGE].apply(generate)\n",
    "if WITHOUT_聖經:\n",
    "    test_df.to_csv(f\"output/iCorpus_test_sample100_without_聖經_Source{SOURCE_LANGUAGE}_TARGET{TARGET_LANGUAGE}.csv\", index = False)\n",
    "else:\n",
    "    test_df.to_csv(f\"output/iCorpus_test_sample100_Source{SOURCE_LANGUAGE}_TARGET{TARGET_LANGUAGE}.csv\", index = False)\n",
    "t = test_df['翻譯'+TARGET_LANGUAGE]\n",
    "c = test_df[SOURCE_LANGUAGE]\n",
    "t = [\" \".join([word for word in sentence]) for sentence in t]\n",
    "c = [[\" \".join([word for word in sentence])] for sentence in c]\n",
    "\n",
    "print(f\"COmputing the BLEU Score ({SOURCE_LANGUAGE} to {TARGET_LANGUAGE}):\")\n",
    "bleu_score = compute_batch_sentence_BLEU(t, c)\n",
    "chrf_score = compute_batch_sentence_CHRF_plus_plus(t, c) / 100\n",
    "print(f\"BLEU:   {bleu_score:.5f}\\nCHRF++: {chrf_score:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
