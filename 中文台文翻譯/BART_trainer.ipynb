{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and check libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#%ip install accelerate -U\n",
    "#%pip install -U transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import regex\n",
    "import re\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    EarlyStoppingCallback,\n",
    "    BartForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq)\n",
    "from itertools import zip_longest\n",
    "from kesi import Ku\n",
    "from StarCC import PresetConversion\n",
    "from datasets import Dataset\n",
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"台文\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16873\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>中文</th>\n",
       "      <th>台文</th>\n",
       "      <th>台羅</th>\n",
       "      <th>白話字</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>人家說當官如果清廉，生活就會清苦</td>\n",
       "      <td>人講做官若清廉，食飯著攪鹽。</td>\n",
       "      <td>Lâng kóng tsò-kuann nā tshing-liâm, tsia̍h-pn̄...</td>\n",
       "      <td>Lâng kóng chò-koaⁿ nā chheng-liâm, chia̍h-pn̄g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>頭髮留那麼長還不剪，莫非是想把錢省下來買花生糖吃。</td>\n",
       "      <td>頭毛留長長，儉錢食塗豆糖。</td>\n",
       "      <td>Thâu-mn̂g lâu tn̂g-tn̂g, khiām-tsînn tsia̍h th...</td>\n",
       "      <td>Thâu-mn̂g lâu tn̂g-tn̂g, khiām-chîⁿ chia̍h thô...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>這項買賣，一邊要，一邊不要。</td>\n",
       "      <td>這項買賣，一頭欲，一頭毋。</td>\n",
       "      <td>Tsit hāng bé-bē, tsi̍t thâu beh, tsi̍t thâu m̄.</td>\n",
       "      <td>Chit hāng bé-bē, chi̍t thâu beh, chi̍t thâu m̄.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>大事化小，小事化無。</td>\n",
       "      <td>大事化小事，小事化無事。</td>\n",
       "      <td>Tuā-sū huà sió-sū, sió-sū huà bô sū.</td>\n",
       "      <td>Tōa-sū hòa sió-sū, sió-sū hòa bô sū.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>北部、南部都很知名。</td>\n",
       "      <td>頂港有名聲，下港上出名。</td>\n",
       "      <td>Tíng-káng ū miâ-siann, ē-káng siōng tshut-miâ.</td>\n",
       "      <td>Téng-káng ū miâ-siaⁿ, ē-káng siōng chhut-miâ.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          中文              台文  \\\n",
       "0           人家說當官如果清廉，生活就會清苦  人講做官若清廉，食飯著攪鹽。   \n",
       "1  頭髮留那麼長還不剪，莫非是想把錢省下來買花生糖吃。   頭毛留長長，儉錢食塗豆糖。   \n",
       "2             這項買賣，一邊要，一邊不要。   這項買賣，一頭欲，一頭毋。   \n",
       "3                 大事化小，小事化無。    大事化小事，小事化無事。   \n",
       "4                 北部、南部都很知名。    頂港有名聲，下港上出名。   \n",
       "\n",
       "                                                  台羅  \\\n",
       "0  Lâng kóng tsò-kuann nā tshing-liâm, tsia̍h-pn̄...   \n",
       "1  Thâu-mn̂g lâu tn̂g-tn̂g, khiām-tsînn tsia̍h th...   \n",
       "2    Tsit hāng bé-bē, tsi̍t thâu beh, tsi̍t thâu m̄.   \n",
       "3               Tuā-sū huà sió-sū, sió-sū huà bô sū.   \n",
       "4     Tíng-káng ū miâ-siann, ē-káng siōng tshut-miâ.   \n",
       "\n",
       "                                                 白話字  \n",
       "0  Lâng kóng chò-koaⁿ nā chheng-liâm, chia̍h-pn̄g...  \n",
       "1  Thâu-mn̂g lâu tn̂g-tn̂g, khiām-chîⁿ chia̍h thô...  \n",
       "2    Chit hāng bé-bē, chi̍t thâu beh, chi̍t thâu m̄.  \n",
       "3               Tōa-sū hòa sió-sū, sió-sū hòa bô sū.  \n",
       "4      Téng-káng ū miâ-siaⁿ, ē-káng siōng chhut-miâ.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df2 = pd.read_csv(\"data/moedict_平行_人工調整.csv\")\n",
    "df3 = pd.read_csv(\"../平行語料/聖經平行語料_p_final.csv\")\n",
    "df5 = pd.read_csv(\"../平行語料/TAT_p.csv\")\n",
    "df = pd.concat([df2, df5], axis=0).drop_duplicates()\n",
    "df = df[['中文', '台文', '台羅', '白話字']]\n",
    "print(df.shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16873, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 轉白話字或漢羅\n",
    "#%pip install KeSi\n",
    "def lomaji2POJ(lomaji)->str:\n",
    "    '''\n",
    "    轉白話字\n",
    "    '''\n",
    "    ji_ls = re.split(r' |\\xa0|\\u3000', str(lomaji))\n",
    "    trans_ji = ' '.join([Ku(ji).POJ().hanlo for ji in ji_ls])\n",
    "    return trans_ji\n",
    "def lomaji2KIP(lomaji)->str:\n",
    "    '''\n",
    "    轉羅馬字\n",
    "    '''\n",
    "    ji_ls = re.split(r' |\\xa0|\\u3000', str(lomaji))\n",
    "    trans_ji = ' '.join([Ku(ji).KIP().hanlo for ji in ji_ls])\n",
    "    return trans_ji\n",
    "\n",
    "def fill_na(row):\n",
    "    if pd.isnull(row['台羅']):\n",
    "        row['台羅'] = lomaji2KIP(row['白話字'])\n",
    "    if pd.isnull(row['白話字']):\n",
    "        row['白話字'] = lomaji2POJ(row['台羅'])\n",
    "    return row\n",
    "\n",
    "df = df.apply(fill_na, axis=1).dropna()\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15925, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat the sentence\n",
    "中文 = []\n",
    "台文 = []\n",
    "台羅 = []\n",
    "白話字 = []\n",
    "conti_suffixes = [\"，\", \"、\", \"；\"]\n",
    "end_suffixes = [\"。\",\"？\",\"！\",\"」\",\"）\",\")\",\"!\",\"?\"]\n",
    "ch,tai,tailo,poj = \"\",\"\",\"\",\"\"\n",
    "for i in range(len(df)):\n",
    "    sen = df['中文'].iloc[i]\n",
    "    if any(sen.rstrip().endswith(suffix) for suffix in conti_suffixes):\n",
    "        ch   +=df['中文'].iloc[i].rstrip()\n",
    "        tai  +=df['台文'].iloc[i].rstrip()\n",
    "        tailo+=df['台羅'].iloc[i].rstrip()\n",
    "        poj  +=df['白話字'].iloc[i].rstrip()\n",
    "    if any(sen.rstrip().endswith(suffix) for suffix in end_suffixes):\n",
    "        ch   +=df['中文'].iloc[i].rstrip()\n",
    "        tai  +=df['台文'].iloc[i].rstrip()\n",
    "        tailo+=df['台羅'].iloc[i].rstrip()\n",
    "        poj  +=df['白話字'].iloc[i].rstrip()\n",
    "        中文.append(ch)\n",
    "        台文.append(tai)\n",
    "        台羅.append(tailo)\n",
    "        白話字.append(poj)\n",
    "        ch,tai,tailo,poj = \"\",\"\",\"\",\"\"\n",
    "    if not any(sen.rstrip().endswith(suffix) for suffix in conti_suffixes+end_suffixes):\n",
    "        中文.append(df['中文'].iloc[i].rstrip())\n",
    "        台文.append(df['台文'].iloc[i].rstrip())\n",
    "        台羅.append(df['台羅'].iloc[i].rstrip())\n",
    "        白話字.append(df['白話字'].iloc[i].rstrip())\n",
    "df =pd.DataFrame({\n",
    "    \"中文\":中文,\n",
    "    \"台文\":台文,\n",
    "    \"台羅\":台羅,\n",
    "    \"白話字\":白話字\n",
    "})\n",
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df, random_state=46)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "遮的錢是我該當出的份額。\n",
      "Tsia-ê tsînn sī guá kai-tong tshut ê hūn-gia̍h.\n",
      "遮的/錢/是/我/該當/出/的/份額/。\n",
      "我做甲流汗，猶閣予人嫌甲流瀾，有夠慼心啦！\n",
      "Guá tsò kah lâu-kuānn, iáu-koh hōo lâng hiâm kah lâu-nuā, ū-kàu tsheh-sim--lah!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我/做/甲/流汗/，/猶/閣予/人/嫌/甲/流/瀾，/有/夠/慼心/啦！'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_tw2cn = PresetConversion(src='tw', dst='cn', with_phrase=False)\n",
    "\n",
    "def segmentation(例句, 例句標音):\n",
    "    '''\n",
    "    ```python\n",
    "    >>> 例句 = '塗跤一半擺仔無拭，袂偌垃圾啦！'\n",
    "    >>> 例句標音 = 'Thôo-kha tsi̍t-puànn-pái-á bô tshit, bē guā lah-sap--lah!'\n",
    "    >>> segmentation(例句, 例句標音)\n",
    "    '塗跤|一半擺仔|無|拭|，|袂|偌|垃圾啦|！'\n",
    "    ```\n",
    "    '''\n",
    "    s = re.sub('--','-',例句標音)\n",
    "    s = re.sub(',|，',', ',例句標音)\n",
    "    #s = re.sub('',', ',例句標音)\n",
    "    #s = re.sub('「|」', ' inn ',例句標音)\n",
    "    parts = regex.split(r' |(?=-)|(?=\\p{Punct})', s)\n",
    "    pattern = regex.compile(r'^[a-zA-Z]')\n",
    "\n",
    "    split_sentence = [i for i in 例句]\n",
    "    #合併英文名字\n",
    "    sentence = []\n",
    "    english_word = \"\"\n",
    "    for character in split_sentence:\n",
    "        if ord(character) < 128 and character != \" \":\n",
    "            english_word += character\n",
    "        else:\n",
    "            if len(english_word)!=0:\n",
    "                sentence.append(english_word)\n",
    "                english_word = \"\"\n",
    "            sentence.append(character)\n",
    "\n",
    "    res = []\n",
    "    for (ch, part) in zip_longest(sentence, parts):\n",
    "        if ch is None:\n",
    "            break\n",
    "        if part is None:  # Handle the case where there's no corresponding part for a character\n",
    "            res.append(ch)\n",
    "        elif not part.startswith('-'): # also detect ch is not start with English character\n",
    "            res.append('/')\n",
    "        res.append(ch)\n",
    "    return ''.join(res[1:])\n",
    "\n",
    "i = 1351\n",
    "print(df['台文'].iloc[i])\n",
    "print(df['台羅'].iloc[i])\n",
    "print(segmentation(df['台文'].iloc[i], df['台羅'].iloc[i]))\n",
    "j = 2902\n",
    "print(df['台文'].iloc[j])\n",
    "print(df['台羅'].iloc[j])\n",
    "segmentation(df['台文'].iloc[j], df['台羅'].iloc[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "華語 = []\n",
    "台語 =[]\n",
    "华语 = []\n",
    "台语 = []\n",
    "d = {}\n",
    "for i in range(len(df)):\n",
    "    例句標音 = df[\"台羅\"].iloc[i].rstrip()\n",
    "    #例句 = segmentation(df[\"台文\"].iloc[i].rstrip(), 例句標音)\n",
    "    例句 = df[\"台文\"].iloc[i].rstrip()\n",
    "    if 例句[-1] != 例句[-2] and (LANGUAGE == \"台文\" or LANGUAGE == \"中文\"):\n",
    "        華語.append(df[\"中文\"].iloc[i].rstrip())\n",
    "        台語.append(例句)\n",
    "    else:\n",
    "        華語.append(df[\"中文\"].iloc[i].rstrip())\n",
    "        台語.append(例句)\n",
    "for 華, 台 in zip(華語, 台語):\n",
    "    华语.append(convert_tw2cn(華))\n",
    "    台语.append(convert_tw2cn(台))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of vocab:25643\n"
     ]
    }
   ],
   "source": [
    "with open('data/vocab.txt', 'r', encoding='utf-8') as file:\n",
    "    vocab_list = [vocab.rstrip() for vocab in file.readlines()]\n",
    "print(\"number of vocab:{}\".format(len(vocab_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AddedToken, PreTrainedTokenizer\n",
    "from typing import Union, Optional\n",
    "from os import makedirs\n",
    "from os.path import join\n",
    "class CharBasedTokeniser(PreTrainedTokenizer):\n",
    "\n",
    "    model_input_names = ['input_ids', 'attention_mask']\n",
    "\n",
    "    def __init__(self, vocab: Union[list[str], str]) -> None:\n",
    "        if isinstance(vocab, str):\n",
    "            with open(vocab, encoding='utf-8') as f:\n",
    "                vocab = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "        super().__init__(\n",
    "            pad_token=AddedToken('[PAD]'),\n",
    "            unk_token=AddedToken('[UNK]'),\n",
    "            bos_token=AddedToken('[BOS]'),\n",
    "            eos_token=AddedToken('[EOS]'),\n",
    "            mask_token=AddedToken('[MSK]'),\n",
    "        )\n",
    "\n",
    "        assert vocab[0] == '[PAD]'\n",
    "        assert vocab[1] == '[UNK]'\n",
    "        assert vocab[2] == '[BOS]'\n",
    "        assert vocab[3] == '[EOS]'\n",
    "        assert vocab[4] == '[MSK]'\n",
    "\n",
    "        self.special_tokens_encoder = {\n",
    "            self.pad_token: 0,\n",
    "            self.unk_token: 1,\n",
    "            self.bos_token: 2,\n",
    "            self.eos_token: 3,\n",
    "            self.mask_token: 4,\n",
    "        }\n",
    "        self._num_special_tokens = len(self.special_tokens_encoder)\n",
    "        self.special_tokens_decoder = {v: k for k, v in self.special_tokens_encoder.items()}\n",
    "\n",
    "        self.id2ch = vocab\n",
    "        self.ch2id = {c: i for i, c in enumerate(vocab)}\n",
    "\n",
    "        assert self.pad_token_id == 0\n",
    "        assert self.unk_token_id == 1\n",
    "        assert self.bos_token_id == 2\n",
    "        assert self.eos_token_id == 3\n",
    "        assert self.mask_token_id == 4\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.id2ch)\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0: list[int], token_ids_1) -> list[int]:\n",
    "        assert token_ids_1 is None\n",
    "        return [self.bos_token_id, *token_ids_0, self.eos_token_id]\n",
    "\n",
    "    def _tokenize(self, text: str) -> list[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        unk_token_id = self.ch2id[self.unk_token]\n",
    "        return self.ch2id.get(token, unk_token_id)\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        return self.id2ch[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        while tokens and tokens[-1] == '[PAD]':\n",
    "            tokens.pop()\n",
    "        if tokens and tokens[0] == '[BOS]':\n",
    "            tokens = tokens[1:]\n",
    "        if tokens and tokens[-1] == '[EOS]':\n",
    "            tokens.pop()\n",
    "        return ''.join(tokens)\n",
    "\n",
    "    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n",
    "        filename = 'vocab.txt'\n",
    "        if filename_prefix is not None:\n",
    "            filename = filename_prefix + filename\n",
    "        if save_directory is not None:\n",
    "            makedirs(save_directory, exist_ok=True)\n",
    "            filename = join(save_directory, filename)\n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for w in self.id2ch:\n",
    "                print(w, file=f)\n",
    "\n",
    "        return (filename,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(language:str, type:str, vocab_list = None):\n",
    "    if type == \"sentenepiece\":\n",
    "        Hokkien_sp_model_file = \"sentencepiece/\"+ language +\"tokenizer.model\"\n",
    "        Hokkien_sp_model = spm.SentencePieceProcessor()\n",
    "        Hokkien_sp_model.Load(Hokkien_sp_model_file)\n",
    "        Hokkien_spm = sp_pb2_model.ModelProto()\n",
    "        Hokkien_spm.ParseFromString(Hokkien_sp_model.serialized_model_proto())\n",
    "        new_vocab = sorted([p.piece for p in Hokkien_spm.pieces[3:]], key=lambda x: len(x),  reverse=True)\n",
    "        return len(new_vocab), Hokkien_sp_model\n",
    "        \n",
    "    elif type == \"CharBasedTokeniser\" and language != \"台文\":\n",
    "        word_list = [re.sub(r\"！|!|_|＿|\\?|？|\\.|。|;|:|\\(|\\)\", \"\", word) for sen in df[language] for word in regex.split(r' |(?=-)|(?=\\p{Punct})', sen)]\n",
    "        word_list = [word.split('\\n') for word in word_list]\n",
    "        word_list = [i for word in word_list for i in word]\n",
    "        word_list = [re.sub(r\"-+|──\", \"-\", word) for word in word_list]+[\"！\",\"!\",\"_\"\",\",\"＿\",\"?\",\"？\",\".\",\"。\",\";\",\":\",\"(\",\")\"]\n",
    "        new_vocab = [i for i in set(word_list)-set(vocab_list)]\n",
    "        vocab_list = vocab_list+sorted(new_vocab, key=lambda x: len(x),  reverse=True)\n",
    "        tokenizer = CharBasedTokeniser(vocab = vocab_list)\n",
    "        return tokenizer.vocab_size, tokenizer\n",
    "\n",
    "    elif type == \"LLamaTokenizer\":\n",
    "        from transformers import LlamaTokenizer\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(\"tokenizer/\"+language+\"/translator_tokenizer_hf\")\n",
    "        return tokenizer.vocab_size+3, tokenizer   # [BOS] [EOS] [PAD]\n",
    "               \n",
    "    else:\n",
    "        # default\n",
    "        tokenizer = CharBasedTokeniser(vocab = vocab_list)\n",
    "        return tokenizer.vocab_size, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of vocab before tokenize:25643\n"
     ]
    }
   ],
   "source": [
    "if LANGUAGE == \"台文\":\n",
    "    vocab_length, tokenizer = get_tokenizer(LANGUAGE, \"CharBasedTokeniser\", vocab_list = vocab_list) \n",
    "    print(\"number of vocab before tokenize:{}\".format(tokenizer.vocab_size))\n",
    "elif LANGUAGE == \"台羅\":\n",
    "    vocab_length, tokenizer = get_tokenizer(LANGUAGE, \"CharBasedTokeniser\", vocab_list = vocab_list)\n",
    "    #print(\"number of vocab before tokenize:{}\".format(tokenizer.vocab_size))\n",
    "elif LANGUAGE == \"白話字\":\n",
    "    vocab_length, tokenizer = get_tokenizer(LANGUAGE, \"LLamaTokenizer\", vocab_list = vocab_list)\n",
    "    #print(\"number of vocab before tokenize: {}\".format(tokenizer.vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "火車敗馬去，死傷真濟人。\n",
      "['火', '車', '敗', '馬', '去', '，', '死', '傷', '真', '濟', '人', '。']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"台文\"].iloc[0])\n",
    "print(tokenizer.tokenize(df[\"台文\"].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# train test split\n",
    "len_dataset = len(華語)\n",
    "len_train = math.floor(len_dataset * 0.8)\n",
    "len_dev = math.floor(len_dataset * 0.1)\n",
    "len_test = len_dataset - len_train - len_dev\n",
    "\n",
    "# adjust the model max length and tokenzier padding max length\n",
    "max_len_hokkien = max(len(tokenizer.tokenize(l)) for l in 台语) + 2 \n",
    "min_len_hokkien = min(len(tokenizer.tokenize(l)) for l in 台语) + 2 \n",
    "\n",
    "if LANGUAGE == \"台文\":\n",
    "    max_len_mandarin = max(len(tokenizer.tokenize(l)) for l in 华语) + 2 # 2: [BOS] and [EOS]\n",
    "    min_len_mandarin = min(len(tokenizer.tokenize(l)) for l in 华语) + 2 # 2: [BOS] and [EOS]\n",
    "    max_length = min(max(max_len_mandarin, max_len_hokkien), 200)\n",
    "    min_length = min(min_len_mandarin, min_len_hokkien)\n",
    "elif LANGUAGE == \"台羅\":\n",
    "    max_len_kip = max(len(tokenizer.tokenize(l)) for l in df['台羅']) + 2 \n",
    "    min_len_kip = min(len(tokenizer.tokenize(l)) for l in df['台羅']) + 2 \n",
    "    max_length = min(max(min_len_hokkien, max_len_kip), 200)\n",
    "    min_length = min(max_len_hokkien, min_len_kip)\n",
    "elif LANGUAGE == \"白話字\":\n",
    "    max_len_poj = max(len(tokenizer.tokenize(l)) for l in df['白話字']) + 2 \n",
    "    min_len_poj = min(len(tokenizer.tokenize(l)) for l in df['白話字']) + 2 \n",
    "    max_length = min(max(max_len_hokkien, max_len_poj), 200)\n",
    "    min_length = min(max_len_hokkien, min_len_poj)\n",
    "\n",
    "print(max_length)\n",
    "print(min_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LANGUAGE == \"台文\":\n",
    "    source_language = 華語\n",
    "    target_language = 台語\n",
    "elif LANGUAGE == \"台羅\":\n",
    "    assert len(华语) == len(df['台羅'])\n",
    "    source_language = 台语\n",
    "    target_language = [re.sub(r\"-+|──\", \"-\", word) for word in df['台羅']]\n",
    "elif LANGUAGE == \"白話字\":\n",
    "    assert len(华语) == len(df['白話字'])\n",
    "    source_language = 台语\n",
    "    target_language = [re.sub(r\"-+|──\", \"-\", word) for word in df['白話字']]\n",
    "\n",
    "train_data_txt = Dataset.from_dict({\n",
    "    \"source language\": source_language[:len_train],\n",
    "    \"target language\": target_language[:len_train]\n",
    "})\n",
    "validation_data_txt = Dataset.from_dict({\n",
    "    \"source language\": source_language[len_train:],\n",
    "    \"target language\": target_language[len_train:]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16676a944e8849a287a18290777b191e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12740 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e01f9ed41cf49e596aa334ef79c85f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3185 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 12740\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_tokenize_preprocess(batch, tokenizer, max_length):\n",
    "    source, target = batch['source language'], batch['target language']\n",
    "    \n",
    "    source_tokenized = tokenizer(\n",
    "        source, padding=\"max_length\", truncation=True, max_length=max_length\n",
    "    )\n",
    "    target_tokenized = tokenizer(\n",
    "        target, padding=\"max_length\", truncation=True, max_length=max_length\n",
    "    )\n",
    "\n",
    "    batch = {k: v for k, v in source_tokenized.items()}\n",
    "    # Ignore padding in the loss\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in l]\n",
    "        for l in target_tokenized[\"input_ids\"]\n",
    "    ]\n",
    "    return batch\n",
    "\n",
    "train_data = train_data_txt.map(\n",
    "    lambda batch: batch_tokenize_preprocess(\n",
    "        batch, tokenizer, max_length\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=train_data_txt.column_names,\n",
    ")\n",
    "\n",
    "validation_data = validation_data_txt.map(\n",
    "    lambda batch: batch_tokenize_preprocess(\n",
    "        batch, tokenizer, max_length\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=validation_data_txt.column_names,\n",
    ")\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS:  2\n",
      "EOS:  3\n",
      "PAD:  0\n",
      "MASK:  4\n",
      "UNK:  1\n",
      "Decoder Start Token ID:  2\n",
      "Forced EOS Token ID:  3\n",
      "Decode max_length:  200\n"
     ]
    }
   ],
   "source": [
    "# special tokens map to model config\n",
    "# for tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained(\"fnlp/bart-base-chinese\")\n",
    "model.config.bos_token_id = tokenizer.bos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.mask_token_id = tokenizer.mask_token_id\n",
    "model.config.unk_token_id = tokenizer.unk_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "print(\"BOS: \", model.config.bos_token_id)\n",
    "print(\"EOS: \",model.config.eos_token_id)\n",
    "print(\"PAD: \",model.config.pad_token_id)\n",
    "print(\"MASK: \",model.config.mask_token_id)\n",
    "print(\"UNK: \",model.config.unk_token_id)\n",
    "print(\"Decoder Start Token ID: \",model.config.decoder_start_token_id)\n",
    "print(\"Forced EOS Token ID: \",model.config.forced_eos_token_id)\n",
    "# error here!!!!\n",
    "model.resize_token_embeddings(vocab_length)\n",
    "# for decode max_length\n",
    "model.config.task_specific_params = {\"translation\": {\n",
    "  \"length_penalty\": 1.0,\n",
    "  \"max_length\": max_length,   # Adjust based on the expected translation length\n",
    "  \"min_length\": min_length,   # Adjust based on the expected translation length\n",
    "  \"num_beams\": 4              # Experiment with this value\n",
    "}}\n",
    "model.config.max_length = max_length\n",
    "#model.config.max_length = 200\n",
    "print(\"Decode max_length: \",model.config.max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "#evaluate.list_evaluation_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metric():\n",
    "    decoded_preds = ['好機會']\n",
    "    decoded_labels = ['好字運']\n",
    "    c = chrf.compute(predictions=[\"\".join(tokenizer.tokenize(pred)) for pred in decoded_preds], references=[\"\".join(tokenizer.tokenize(label)) for label in decoded_labels], word_order=2)\n",
    "    b = bleu.compute(predictions=[\" \".join(tokenizer.tokenize(pred)) for pred in decoded_preds], references=[\" \".join(tokenizer.tokenize(label)) for label in decoded_labels], smooth=True)\n",
    "    c.update(b)\n",
    "    del c['precisions']\n",
    "    c['score'] = c['score'] / 100\n",
    "    return {\n",
    "        \"CHRF++\": c['score'],\n",
    "        \"BLEU Score\": c['bleu']\n",
    "    }\n",
    "#test_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.520 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "convert_cn2tw = PresetConversion(src='cn', dst='tw', with_phrase=False)\n",
    "\n",
    "def process_text_evaluate(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_bleu(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = process_text_evaluate(decoded_preds, decoded_labels)\n",
    "    # Compute CHRF++ scores with (word_order=2, lowercase=True)\n",
    "    chrf_results = chrf.compute(predictions=[\"\".join(tokenizer.tokenize(pred)) for pred in decoded_preds], references=[\"\".join(tokenizer.tokenize(label)) for label in decoded_labels], word_order=2)\n",
    "    # Compute BLEU scores\n",
    "    bleu_results = bleu.compute(predictions=[\" \".join(tokenizer.tokenize(pred)) for pred in decoded_preds], references=[\" \".join(tokenizer.tokenize(label)) for label in decoded_labels], smooth=True)\n",
    "    chrf_results.update(bleu_results)\n",
    "    del chrf_results['precisions']\n",
    "    chrf_results['score'] = chrf_results['score'] / 100\n",
    "    return {\n",
    "        \"CHRF++\": chrf_results['score'],\n",
    "        \"BLEU Score\": chrf_results['bleu']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is save at: model/台文/2023_9_11_11_57_maxLength200\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "formatted_datetime = (str(datetime.now().year)+\"_\"+\n",
    "                      str(datetime.now().month)+\"_\"+\n",
    "                      str(datetime.now().day)+\"_\"+\n",
    "                      str(datetime.now().hour)+\"_\"+\n",
    "                      str(datetime.now().minute)+\"_maxLength\"+\n",
    "                      str(model.config.max_length))\n",
    "                      \n",
    "if LANGUAGE == \"台文\":\n",
    "    output_dir = \"model/台文/\" + formatted_datetime\n",
    "elif LANGUAGE == \"台羅\":\n",
    "    output_dir = \"model/台羅/\" + formatted_datetime\n",
    "elif LANGUAGE == \"白話字\":\n",
    "    output_dir = \"model/白話字/\" + formatted_datetime\n",
    "print(f\"model is save at: {output_dir}\")\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=30,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=40, \n",
    "    per_device_eval_batch_size=40,\n",
    "    learning_rate=3e-04,\n",
    "    warmup_steps=2000,\n",
    "    weight_decay=0.05,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length = model.config.max_length, # max length for best bleu score\n",
    "    save_total_limit=2,\n",
    "    save_strategy='epoch',\n",
    "    evaluation_strategy=\"epoch\", # Specify the evaluation strategy based on steps\n",
    "    load_best_model_at_end = True # this will let the model save the best checkpoint\n",
    "    # no_cuda = True    \n",
    ")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_bleu,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoeyliang\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/u5110390/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/u5110390/BART_pytorch/wandb/run-20230911_115730-kpbpkzvz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joeyliang/bart_wiki_lingua/runs/kpbpkzvz' target=\"_blank\">charmed-flower-89</a></strong> to <a href='https://wandb.ai/joeyliang/bart_wiki_lingua' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joeyliang/bart_wiki_lingua' target=\"_blank\">https://wandb.ai/joeyliang/bart_wiki_lingua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joeyliang/bart_wiki_lingua/runs/kpbpkzvz' target=\"_blank\">https://wandb.ai/joeyliang/bart_wiki_lingua/runs/kpbpkzvz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WANDB_INTEGRATION = True\n",
    "if WANDB_INTEGRATION:\n",
    "    import wandb\n",
    "    wandb.login(key=\"f65de74e8b17eee687fc6b6a91dc155659e02a5d\")\n",
    "from datetime import datetime\n",
    "\n",
    "if WANDB_INTEGRATION:\n",
    "    wandb_run = wandb.init(\n",
    "        project=\"bart_wiki_lingua\",\n",
    "        config={\n",
    "            \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "            \"learning_rate\": training_args.learning_rate\n",
    "        },\n",
    "    )\n",
    "\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H%M%S\")\n",
    "    wandb_run.name = \"run_\" + \"Chinese-Hokkien\" + \"_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"fnlp/bart-base-chinese\",\n",
       "  \"activation_dropout\": 0.1,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 12,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 12,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 3,\n",
       "  \"forced_eos_token_id\": 3,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"mask_token_id\": 4,\n",
       "  \"max_length\": 200,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 4,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"scale_embedding\": false,\n",
       "  \"task_specific_params\": {\n",
       "    \"translation\": {\n",
       "      \"length_penalty\": 1.0,\n",
       "      \"max_length\": 200,\n",
       "      \"min_length\": 3,\n",
       "      \"num_beams\": 4\n",
       "    }\n",
       "  },\n",
       "  \"tokenizer_class\": \"BertTokenizer\",\n",
       "  \"transformers_version\": \"4.31.0\",\n",
       "  \"unk_token_id\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 25643\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 11 11:57:39 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 12.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:1C:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    57W / 300W |    999MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u5110390/.local/lib/python3.11/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/80 00:12 < 15:42, 0.08 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _xla_gc_callback at 0x7fab0a772480>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/u5110390/.local/lib/python3.11/site-packages/jax/_src/lib/__init__.py\", line 98, in _xla_gc_callback\n",
      "    def _xla_gc_callback(*args):\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prediction \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mpredict(validation_data)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(prediction\u001b[39m.\u001b[39mmetrics)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(prediction\u001b[39m.\u001b[39mpredictions\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer_seq2seq.py:216\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m gen_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m (\n\u001b[1;32m    212\u001b[0m     gen_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m gen_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgeneration_num_beams\n\u001b[1;32m    213\u001b[0m )\n\u001b[1;32m    214\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gen_kwargs \u001b[39m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mpredict(test_dataset, ignore_keys\u001b[39m=\u001b[39mignore_keys, metric_key_prefix\u001b[39m=\u001b[39mmetric_key_prefix)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:3010\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3007\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3009\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3010\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   3011\u001b[0m     test_dataloader, description\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPrediction\u001b[39m\u001b[39m\"\u001b[39m, ignore_keys\u001b[39m=\u001b[39mignore_keys, metric_key_prefix\u001b[39m=\u001b[39mmetric_key_prefix\n\u001b[1;32m   3012\u001b[0m )\n\u001b[1;32m   3013\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   3014\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:3123\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3120\u001b[0m         batch_size \u001b[39m=\u001b[39m observed_batch_size\n\u001b[1;32m   3122\u001b[0m \u001b[39m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3123\u001b[0m loss, logits, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_step(model, inputs, prediction_loss_only, ignore_keys\u001b[39m=\u001b[39mignore_keys)\n\u001b[1;32m   3124\u001b[0m inputs_decode \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_input(inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39minclude_inputs_for_metrics \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3126\u001b[0m \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer_seq2seq.py:282\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    277\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inputs\n\u001b[1;32m    278\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdecoder_input_ids\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inputs\n\u001b[1;32m    279\u001b[0m     \u001b[39mand\u001b[39;00m inputs[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m inputs[\u001b[39m\"\u001b[39m\u001b[39mdecoder_input_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape\n\u001b[1;32m    280\u001b[0m ):\n\u001b[1;32m    281\u001b[0m     inputs \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdecoder_input_ids\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m--> 282\u001b[0m generated_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgen_kwargs)\n\u001b[1;32m    284\u001b[0m \u001b[39m# Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[39m# TODO: remove this hack when the legacy code that initializes generation_config from a model config is\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39m# removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgeneration_config\u001b[39m.\u001b[39m_from_model_config:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/utils.py:1627\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1620\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1621\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1622\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1623\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1624\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1625\u001b[0m     )\n\u001b[1;32m   1626\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1627\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_search(\n\u001b[1;32m   1628\u001b[0m         input_ids,\n\u001b[1;32m   1629\u001b[0m         beam_scorer,\n\u001b[1;32m   1630\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m   1631\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1632\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[1;32m   1633\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[1;32m   1634\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[1;32m   1635\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1636\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[1;32m   1637\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1638\u001b[0m     )\n\u001b[1;32m   1640\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1641\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1642\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/utils.py:2951\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2946\u001b[0m next_token_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madjust_logits_during_generation(next_token_logits, cur_len\u001b[39m=\u001b[39mcur_len)\n\u001b[1;32m   2947\u001b[0m next_token_scores \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlog_softmax(\n\u001b[1;32m   2948\u001b[0m     next_token_logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m   2949\u001b[0m )  \u001b[39m# (batch_size * num_beams, vocab_size)\u001b[39;00m\n\u001b[0;32m-> 2951\u001b[0m next_token_scores_processed \u001b[39m=\u001b[39m logits_processor(input_ids, next_token_scores)\n\u001b[1;32m   2952\u001b[0m next_token_scores \u001b[39m=\u001b[39m next_token_scores_processed \u001b[39m+\u001b[39m beam_scores[:, \u001b[39mNone\u001b[39;00m]\u001b[39m.\u001b[39mexpand_as(next_token_scores)\n\u001b[1;32m   2954\u001b[0m \u001b[39m# Store scores, attentions and hidden_states when required\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/logits_process.py:97\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     96\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores)\n\u001b[1;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/logits_process.py:509\u001b[0m, in \u001b[0;36mNoRepeatNGramLogitsProcessor.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    507\u001b[0m num_batch_hypotheses \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    508\u001b[0m cur_len \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 509\u001b[0m banned_batch_tokens \u001b[39m=\u001b[39m _calc_banned_ngram_tokens(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngram_size, input_ids, num_batch_hypotheses, cur_len)\n\u001b[1;32m    511\u001b[0m \u001b[39mfor\u001b[39;00m i, banned_tokens \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(banned_batch_tokens):\n\u001b[1;32m    512\u001b[0m     scores[i, banned_tokens] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minf\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/logits_process.py:481\u001b[0m, in \u001b[0;36m_calc_banned_ngram_tokens\u001b[0;34m(ngram_size, prev_input_ids, num_hypos, cur_len)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m<\u001b[39m ngram_size:\n\u001b[1;32m    478\u001b[0m     \u001b[39m# return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\u001b[39;00m\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos)]\n\u001b[0;32m--> 481\u001b[0m generated_ngrams \u001b[39m=\u001b[39m _get_ngrams(ngram_size, prev_input_ids, num_hypos)\n\u001b[1;32m    483\u001b[0m banned_tokens \u001b[39m=\u001b[39m [\n\u001b[1;32m    484\u001b[0m     _get_generated_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n\u001b[1;32m    485\u001b[0m     \u001b[39mfor\u001b[39;00m hypo_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos)\n\u001b[1;32m    486\u001b[0m ]\n\u001b[1;32m    487\u001b[0m \u001b[39mreturn\u001b[39;00m banned_tokens\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/logits_process.py:462\u001b[0m, in \u001b[0;36m_get_ngrams\u001b[0;34m(ngram_size, prev_input_ids, num_hypos)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[39mfor\u001b[39;00m ngram \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[gen_tokens[i:] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ngram_size)]):\n\u001b[1;32m    461\u001b[0m         prev_ngram_tuple \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(ngram[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 462\u001b[0m         generated_ngram[prev_ngram_tuple] \u001b[39m=\u001b[39m generated_ngram\u001b[39m.\u001b[39mget(prev_ngram_tuple, []) \u001b[39m+\u001b[39m [ngram[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]\n\u001b[1;32m    463\u001b[0m \u001b[39mreturn\u001b[39;00m generated_ngrams\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prediction = trainer.predict(validation_data)\n",
    "print(prediction.metrics)\n",
    "print(prediction.predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u5110390/.local/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='167' max='26220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  167/26220 00:58 < 2:33:29, 2.83 it/s, Epoch 0.19/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='437' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 65/437 00:57 < 05:36, 1.11 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.403465986251831,\n",
       " 'eval_bleu': 0.13463581588592002,\n",
       " 'eval_precisions': [0.9842016075820287,\n",
       "  0.9729695034379839,\n",
       "  0.964005314651528,\n",
       "  0.9558593021086024],\n",
       " 'eval_brevity_penalty': 0.13891413282668094,\n",
       " 'eval_length_ratio': 0.3362588654717452,\n",
       " 'eval_translation_length': 133368,\n",
       " 'eval_reference_length': 396623,\n",
       " 'eval_runtime': 437.5751,\n",
       " 'eval_samples_per_second': 19.953,\n",
       " 'eval_steps_per_second': 0.999,\n",
       " 'epoch': 30.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = trainer.predict(validation_data)\n",
    "print(prediction.metrics)\n",
    "print(prediction.predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e77711bd117463488cb4f6b0ea309ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>▁▃▇█████████████████████████████████████</td></tr><tr><td>eval/brevity_penalty</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/length_ratio</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/reference_length</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▂▂▁▁▁▁▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▂</td></tr><tr><td>eval/samples_per_second</td><td>▁▇▇▇▇▇█▇▇██████▇███████▇███████▇██████▇▇</td></tr><tr><td>eval/steps_per_second</td><td>▁▇▇▇▇▇█▇▇██████▇███████▇███████▇██████▇▇</td></tr><tr><td>eval/translation_length</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▃▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>0.13464</td></tr><tr><td>eval/brevity_penalty</td><td>0.13891</td></tr><tr><td>eval/length_ratio</td><td>0.33626</td></tr><tr><td>eval/loss</td><td>1.40347</td></tr><tr><td>eval/reference_length</td><td>396623</td></tr><tr><td>eval/runtime</td><td>437.5751</td></tr><tr><td>eval/samples_per_second</td><td>19.953</td></tr><tr><td>eval/steps_per_second</td><td>0.999</td></tr><tr><td>eval/translation_length</td><td>133368</td></tr><tr><td>train/epoch</td><td>30.0</td></tr><tr><td>train/global_step</td><td>52410</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.352</td></tr><tr><td>train/total_flos</td><td>3.193891028729856e+17</td></tr><tr><td>train/train_loss</td><td>1.47808</td></tr><tr><td>train/train_runtime</td><td>93113.6999</td></tr><tr><td>train/train_samples_per_second</td><td>11.251</td></tr><tr><td>train/train_steps_per_second</td><td>0.563</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">laced-sky-61</strong> at: <a href='https://wandb.ai/joeyliang/bart_wiki_lingua/runs/767aow3y' target=\"_blank\">https://wandb.ai/joeyliang/bart_wiki_lingua/runs/767aow3y</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230821_232252-767aow3y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if WANDB_INTEGRATION:\n",
    "    wandb_run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"source language\"][0:50],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_len_mandarin,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "    \n",
    "def translate_sentence(sentence:str, model)->str:\n",
    "    sen_dataset = Dataset.from_dict({\n",
    "                                    \"source language\": [sentence]\n",
    "                                })\n",
    "    _, translation  = generate_summary(sen_dataset, model)\n",
    "    return _, translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[BOS]警然我佮恁老爸仝年[UNK]，毋过[UNK][UNK]。'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([    2,     2, 19063, 13043,  9448,  5006,  9026, 16474, 13280,  4863,\n",
    "8576,     1, 23709, 11757, 20011,     1,     1,  3322,     3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[    2,     2, 19063, 13043,  9448,  5006,  9026, 16474, 13280,  4863,\n",
      "          8576,     1, 23709, 11757, 20011,     1,     1,  3322,     3]]), ['警然我佮恁老爸仝年，毋过。'])\n",
      "(tensor([[    2,     2, 10226,     1,     1,     3]]), ['散'])\n",
      "(tensor([[    2,     2,  4907, 21919, 17529, 14310,     1,  5169,  5231, 14454,\n",
      "         12136,  7398,  4868, 23709, 11685, 16259,  4646,  7904,     1,     3]]), ['伊食著的候做真济好代，死缝一定'])\n",
      "(tensor([[    2,     2,     1,  4854,  6065,  4656,     1, 14310,     1,  4854,\n",
      "         10627,  5675, 14133,  9238,  3322,     3]]), ['仔口上的仔有刺疼感。'])\n",
      "(tensor([[    2,     2,  4907,  7398, 14275, 21919, 23709,     1,  5759,  6492,\n",
      "             3]]), ['伊好癖食，力喔'])\n",
      "(tensor([[    2,     2,  4828, 10343, 13284,  8940, 23709, 14411, 14411, 14029,\n",
      "         11710,  3322,     3]]), ['人无爽快，直直畏殕。'])\n",
      "(tensor([[    2,     2, 11825,  7329, 11773,     1, 23709, 11757,  5202,  4741,\n",
      "         10421,     1,  7931,  3322,     3]]), ['民头毛，毋值也是家。'])\n"
     ]
    }
   ],
   "source": [
    "model_path = \"model/台文/results_2023-08-15-11-44-02-843691/checkpoint-9500\"\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "sentences = [\n",
    "    \"雖然我和你爸爸同年齡，但是論輩份不論年紀，咱們算同輩，叫我哥哥就好。\",\n",
    "    \"散會時，我們準備了三、四百塊鳳梨酥，在大門口送給客人。\",\n",
    "    \"他活著的時候做了很多好事，死後一定會上天堂。\",\n",
    "    \"傷口在上藥的時候有刺痛感。\",\n",
    "    \"他好幾餐沒吃，餓過頭，沒體力竟暈倒了\",\n",
    "    \"人不舒服，一直感到身體熱起來。\",\n",
    "    \"民主黨在參眾兩院選舉也是贏家，在已開出的票中，民主黨在一百席的參院雖未跨過六十席門檻，但已取得五十六席，在眾院的席次也增為二百五十二席。\"\n",
    "]\n",
    "for sentence in sentences :\n",
    "    translation = translate_sentence(sentence, model)\n",
    "    print(translation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
