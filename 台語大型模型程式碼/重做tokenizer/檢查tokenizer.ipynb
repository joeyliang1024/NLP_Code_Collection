{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a52492e-1eee-4f5d-a219-2166484776a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a793d5-f0e5-4b7c-afb4-80d84ce86f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"ziqingyang/chinese-llama-lora-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "716595d6-6efc-4622-8c0c-8baf5d70c58e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '人工智能', '是', '计算机', '科学', '、', '心理学', '、', '哲学', '等', '学科', '融合', '的', '交叉', '学科', '。']\n"
     ]
    }
   ],
   "source": [
    "#論文句子\n",
    "print(tokenizer.tokenize(\"人工智能是计算机科学、心理学、哲学等学科融合的交叉学科。\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f86be-7181-4c24-835d-8607f4e5b133",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 台文漢字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b39d692-2c76-4648-8741-904f74859292",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "攏是你啦，攏是你啦，攏是你害我來氣身惱命。你的心，我的心，結結做一伙，是欲拍算按怎。\n",
      "['▁', '<0xE6>', '<0x94>', '<0x8F>', '是你', '啦', '，', '<0xE6>', '<0x94>', '<0x8F>', '是你', '啦', '，', '<0xE6>', '<0x94>', '<0x8F>', '是你', '害', '我', '來', '氣', '身', '惱', '命', '。', '你的', '心', '，', '我的', '心', '，', '結', '結', '做', '一', '伙', '，', '是', '欲', '拍', '算', '按', '怎', '。']\n"
     ]
    }
   ],
   "source": [
    "print(\"攏是你啦，攏是你啦，攏是你害我來氣身惱命。你的心，我的心，結結做一伙，是欲拍算按怎。\")\n",
    "print(tokenizer.tokenize(\"攏是你啦，攏是你啦，攏是你害我來氣身惱命。你的心，我的心，結結做一伙，是欲拍算按怎。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c907627-4efe-43e1-80d4-a725320418d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「你攏無變！」「你嘛無變！」「有啦，生兩个矣，變較闊。」「哈！我嘛有啦！變較媠。」欲二十外冬無看著的高中同窗的，和我三个，你一句，我一句。開始滾耍笑了後，時間若像飛轉去阮高中的時，同窗讀冊，做伙講趣味。時間咧變，朋友的感情無變。這二十外冬來，毋是無去想著，只是想著的時，攏是一工無閒了，欲睏的半瞑，定定是孤一人坐佇窗邊寫日記思念。阮三个，一个留佇下港拍拚，一个獨身走國內國外，一个嫁雞綴雞飛、嫁狗綴狗走，對下港離鄉來到臺北城。以前無利便的高鐵，見面比登天較難，有高鐵了後嘛揣無逐家攏有閒的好時機。就按呢，一工過一工、一瞑過一瞑，三个好朋友，三撨四撨，撨二十外冬才會當見一面，閣毋敢耍傷久，干焦耍拜六、禮拜兩工。無煩無惱的青春少女，早就變做為家庭、為囝兒的老母。講到遮，恁應該就知，阮毋是無時間，實在是這二十外冬來，逐家無法度放囝放翁，逐家攏咧等囡仔大漢。阮三个對中晝一見面喙閣毋捌停過，暗時去蹛佇新北投山頂的大飯店，到飯店閣共窗仔開開那講那看淡水河邊、那看城市的燈光閃閃爍爍，講未來、講過去，講甲半瞑。隔轉工，阮過淡水去三芝，雨綿綿，看著櫻花開滿枝，食中晝飽，去石門，揣一間倚海邊的店啉咖啡。阮佇店內揣一位向海的窗仔邊，昨暗阮三个人佇窗邊看城市的閃爍，今仔日阮看波浪拍上海墘。天地遮闊，有緣同窗，有緣相逢，佇通透的窗邊，鼻苦味仔苦味的咖啡，心內煞感覺芳甜仔芳甜。\n",
      "['▁「', '你', '<0xE6>', '<0x94>', '<0x8F>', '無', '變', '！', '」', '「', '你', '嘛', '無', '變', '！', '」', '「', '有', '啦', '，', '生', '兩', '个', '矣', '，', '變', '較', '闊', '。', '」', '「', '哈', '！', '我', '嘛', '有', '啦', '！', '變', '較', '<0xE5>', '<0xAA>', '<0xA0>', '。', '」', '欲', '二十', '外', '冬', '無', '看', '著', '的', '高中', '同', '窗', '的', '，', '和我', '三个', '，', '你', '一句', '，', '我', '一句', '。', '開始', '滾', '耍', '笑', '了', '後', '，', '時間', '若', '像', '飛', '轉', '去', '阮', '高中', '的', '時', '，', '同', '窗', '讀', '冊', '，', '做', '伙', '講', '趣味', '。', '時間', '咧', '變', '，', '朋友', '的感情', '無', '變', '。', '這', '二十', '外', '冬', '來', '，', '毋', '是', '無', '去', '想', '著', '<0xEE>', '<0x9C>', '<0x81>', '，', '只是想', '著', '的', '時', '，', '<0xE6>', '<0x94>', '<0x8F>', '是', '一', '工', '無', '閒', '了', '，', '欲', '<0xE7>', '<0x9D>', '<0x8F>', '的', '半', '瞑', '，', '定', '定', '是', '孤', '一人', '坐', '<0xE4>', '<0xBD>', '<0x87>', '窗', '邊', '寫', '日', '記', '思念', '<0xEE>', '<0x9C>', '<0x81>', '。', '阮', '三个', '，', '一个', '留', '<0xE4>', '<0xBD>', '<0x87>', '下', '港', '拍', '拚', '，', '一个', '獨', '身', '走', '國', '內', '國', '外', '，', '一个', '嫁', '雞', '綴', '雞', '飛', '、', '嫁', '狗', '綴', '狗', '走', '，', '對', '下', '港', '離', '鄉', '來', '到', '臺', '北', '城', '。', '以前', '無', '利', '便', '的', '高', '鐵', '，', '見', '面', '比', '登', '天', '較', '難', '，', '有', '高', '鐵', '了', '後', '嘛', '揣', '無', '逐', '家', '<0xE6>', '<0x94>', '<0x8F>', '有', '閒', '的好', '時', '機', '。', '就', '按', '呢', '，', '一', '工', '過', '一', '工', '、', '一', '瞑', '過', '一', '瞑', '，', '三个', '好朋友', '，', '三', '<0xE6>', '<0x92>', '<0xA8>', '四', '<0xE6>', '<0x92>', '<0xA8>', '，', '<0xE6>', '<0x92>', '<0xA8>', '二十', '外', '冬', '才', '會', '當', '見', '一面', '，', '閣', '毋', '敢', '耍', '傷', '久', '，', '干', '焦', '耍', '拜', '六', '、', '禮', '拜', '兩', '工', '。', '無', '煩', '無', '惱', '的', '青春', '少女', '，', '早就', '變', '做', '為', '家庭', '、', '為', '<0xE5>', '<0x9B>', '<0x9D>', '兒', '的老', '母', '。', '講', '到', '遮', '，', '恁', '應', '該', '就', '知', '，', '阮', '毋', '是', '無', '時間', '，', '實', '在', '是', '這', '二十', '外', '冬', '來', '，', '逐', '家', '無法', '度', '放', '<0xE5>', '<0x9B>', '<0x9D>', '放', '翁', '，', '逐', '家', '<0xE6>', '<0x94>', '<0x8F>', '咧', '等', '囡', '仔', '大', '漢', '。', '阮', '三个', '對', '中', '<0xE6>', '<0x99>', '<0x9D>', '一', '見', '面', '喙', '閣', '毋', '<0xE6>', '<0x8D>', '<0x8C>', '停', '過', '，', '暗', '時', '去', '<0xE8>', '<0xB9>', '<0x9B>', '<0xE4>', '<0xBD>', '<0x87>', '新', '北', '投', '山', '頂', '的大', '飯', '店', '，', '到', '飯', '店', '閣', '共', '窗', '仔', '開', '開', '那', '講', '那', '看', '淡水', '河', '邊', '、', '那', '看', '城市', '的', '燈', '光', '閃', '閃', '<0xE7>', '<0x88>', '<0x8D>', '<0xE7>', '<0x88>', '<0x8D>', '，', '講', '未', '來', '、', '講', '過', '去', '，', '講', '甲', '半', '瞑', '。', '隔', '轉', '工', '，', '阮', '過', '淡水', '去', '三', '芝', '，', '雨', '綿', '綿', '，', '看', '著', '櫻', '花', '開', '滿', '枝', '，', '食', '中', '<0xE6>', '<0x99>', '<0x9D>', '飽', '，', '去', '石', '門', '，', '揣', '一', '間', '倚', '海', '邊', '的', '店', '啉', '咖啡', '。', '阮', '<0xE4>', '<0xBD>', '<0x87>', '店', '內', '揣', '一位', '向', '海', '的', '窗', '仔', '邊', '，', '昨', '暗', '阮', '三个', '人', '<0xE4>', '<0xBD>', '<0x87>', '窗', '邊', '看', '城市', '的', '閃', '<0xE7>', '<0x88>', '<0x8D>', '，', '今', '仔', '日', '阮', '看', '波', '浪', '拍', '上海', '墘', '。', '天地', '遮', '闊', '，', '有', '緣', '同', '窗', '，', '有', '緣', '相', '逢', '，', '<0xE4>', '<0xBD>', '<0x87>', '通', '透', '的', '窗', '邊', '，', '鼻', '苦', '味', '仔', '苦', '味', '的', '咖啡', '，', '心', '內', '煞', '感', '覺', '芳', '甜', '仔', '芳', '甜', '。']\n"
     ]
    }
   ],
   "source": [
    "print(\"「你攏無變！」「你嘛無變！」「有啦，生兩个矣，變較闊。」「哈！我嘛有啦！變較媠。」欲二十外冬無看著的高中同窗的，和我三个，你一句，我一句。開始滾耍笑了後，時間若像飛轉去阮高中的時，同窗讀冊，做伙講趣味。時間咧變，朋友的感情無變。這二十外冬來，毋是無去想著，只是想著的時，攏是一工無閒了，欲睏的半瞑，定定是孤一人坐佇窗邊寫日記思念。阮三个，一个留佇下港拍拚，一个獨身走國內國外，一个嫁雞綴雞飛、嫁狗綴狗走，對下港離鄉來到臺北城。以前無利便的高鐵，見面比登天較難，有高鐵了後嘛揣無逐家攏有閒的好時機。就按呢，一工過一工、一瞑過一瞑，三个好朋友，三撨四撨，撨二十外冬才會當見一面，閣毋敢耍傷久，干焦耍拜六、禮拜兩工。無煩無惱的青春少女，早就變做為家庭、為囝兒的老母。講到遮，恁應該就知，阮毋是無時間，實在是這二十外冬來，逐家無法度放囝放翁，逐家攏咧等囡仔大漢。阮三个對中晝一見面喙閣毋捌停過，暗時去蹛佇新北投山頂的大飯店，到飯店閣共窗仔開開那講那看淡水河邊、那看城市的燈光閃閃爍爍，講未來、講過去，講甲半瞑。隔轉工，阮過淡水去三芝，雨綿綿，看著櫻花開滿枝，食中晝飽，去石門，揣一間倚海邊的店啉咖啡。阮佇店內揣一位向海的窗仔邊，昨暗阮三个人佇窗邊看城市的閃爍，今仔日阮看波浪拍上海墘。天地遮闊，有緣同窗，有緣相逢，佇通透的窗邊，鼻苦味仔苦味的咖啡，心內煞感覺芳甜仔芳甜。\")\n",
    "print(tokenizer.tokenize(\"「你攏無變！」「你嘛無變！」「有啦，生兩个矣，變較闊。」「哈！我嘛有啦！變較媠。」欲二十外冬無看著的高中同窗的，和我三个，你一句，我一句。開始滾耍笑了後，時間若像飛轉去阮高中的時，同窗讀冊，做伙講趣味。時間咧變，朋友的感情無變。這二十外冬來，毋是無去想著，只是想著的時，攏是一工無閒了，欲睏的半瞑，定定是孤一人坐佇窗邊寫日記思念。阮三个，一个留佇下港拍拚，一个獨身走國內國外，一个嫁雞綴雞飛、嫁狗綴狗走，對下港離鄉來到臺北城。以前無利便的高鐵，見面比登天較難，有高鐵了後嘛揣無逐家攏有閒的好時機。就按呢，一工過一工、一瞑過一瞑，三个好朋友，三撨四撨，撨二十外冬才會當見一面，閣毋敢耍傷久，干焦耍拜六、禮拜兩工。無煩無惱的青春少女，早就變做為家庭、為囝兒的老母。講到遮，恁應該就知，阮毋是無時間，實在是這二十外冬來，逐家無法度放囝放翁，逐家攏咧等囡仔大漢。阮三个對中晝一見面喙閣毋捌停過，暗時去蹛佇新北投山頂的大飯店，到飯店閣共窗仔開開那講那看淡水河邊、那看城市的燈光閃閃爍爍，講未來、講過去，講甲半瞑。隔轉工，阮過淡水去三芝，雨綿綿，看著櫻花開滿枝，食中晝飽，去石門，揣一間倚海邊的店啉咖啡。阮佇店內揣一位向海的窗仔邊，昨暗阮三个人佇窗邊看城市的閃爍，今仔日阮看波浪拍上海墘。天地遮闊，有緣同窗，有緣相逢，佇通透的窗邊，鼻苦味仔苦味的咖啡，心內煞感覺芳甜仔芳甜。\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d731a53-e181-4277-a140-b20437e20854",
   "metadata": {},
   "source": [
    "# 白話字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "573a2222-ce09-4416-9287-5df4fcc10f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biō-kong kóng , Thiⁿ-tē hiah-n̍i̍h tōa , chi̍t-ê Siōng-tè beh àn-chóaⁿ liāu-lí ? Chhin-chhiūⁿ kok-ông chi̍t-lâng bē ka-tī liāu-lí , tek-khak tio̍h hong pah-koaⁿ lâi pang-chān , liōng-pit Siōng-tè iā tio̍h ēng só͘ hong ê sîn lâi pang-chō͘ . \n",
      "['▁Bi', 'ō', '-', 'k', 'ong', '▁k', 'ó', 'ng', '▁,', '▁Th', 'i', 'ⁿ', '-', 't', 'ē', '▁h', 'iah', '-', 'n', '̍', 'i', '̍', 'h', '▁t', 'ō', 'a', '▁,', '▁chi', '̍', 't', '-', 'ê', '▁Si', 'ō', 'ng', '-', 't', 'è', '▁beh', '▁à', 'n', '-', 'ch', 'ó', 'a', 'ⁿ', '▁li', 'ā', 'u', '-', 'l', 'í', '▁?', '▁Ch', 'hin', '-', 'ch', 'hi', 'ū', 'ⁿ', '▁k', 'ok', '-', 'ô', 'ng', '▁chi', '̍', 't', '-', 'l', 'â', 'ng', '▁b', 'ē', '▁ka', '-', 't', 'ī', '▁li', 'ā', 'u', '-', 'l', 'í', '▁,', '▁tek', '-', 'kh', 'ak', '▁t', 'io', '̍', 'h', '▁h', 'ong', '▁p', 'ah', '-', 'ko', 'a', 'ⁿ', '▁l', 'â', 'i', '▁p', 'ang', '-', 'ch', 'ā', 'n', '▁,', '▁li', 'ō', 'ng', '-', 'pit', '▁Si', 'ō', 'ng', '-', 't', 'è', '▁i', 'ā', '▁t', 'io', '̍', 'h', '▁', 'ē', 'ng', '▁só', '<0xCD>', '<0x98>', '▁h', 'ong', '▁', 'ê', '▁s', 'în', '▁l', 'â', 'i', '▁p', 'ang', '-', 'ch', 'ō', '<0xCD>', '<0x98>', '▁.', '▁']\n"
     ]
    }
   ],
   "source": [
    "print(\"Biō-kong kóng , Thiⁿ-tē hiah-n̍i̍h tōa , chi̍t-ê Siōng-tè beh àn-chóaⁿ liāu-lí ? Chhin-chhiūⁿ kok-ông chi̍t-lâng bē ka-tī liāu-lí , tek-khak tio̍h hong pah-koaⁿ lâi pang-chān , liōng-pit Siōng-tè iā tio̍h ēng só͘ hong ê sîn lâi pang-chō͘ . \")\n",
    "print(tokenizer.tokenize(\"Biō-kong kóng , Thiⁿ-tē hiah-n̍i̍h tōa , chi̍t-ê Siōng-tè beh àn-chóaⁿ liāu-lí ? Chhin-chhiūⁿ kok-ông chi̍t-lâng bē ka-tī liāu-lí , tek-khak tio̍h hong pah-koaⁿ lâi pang-chān , liōng-pit Siōng-tè iā tio̍h ēng só͘ hong ê sîn lâi pang-chō͘ . \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b9d47-c879-46b6-aeaa-95db4dec7eae",
   "metadata": {},
   "source": [
    "# 漢羅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5257dbf0-7710-4a82-b74d-45c0908604bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "德國前有一個皇帝，真愛相刣，一生常常tī戰場裡，罕得tī皇宮裡。伊的對敵常常kap伊抵硈，無hō͘伊一時仔歇睏。一個hō͘伊圍困了，別個就koh侵入伊的境界。所以年過年，無論冬天，無論熱天，伊著做頭chhōa伊的軍兵，kap in受寒，熱，枵餓，嘴乾的艱苦。\n",
      "['▁', '德國', '前', '有一', '個', '皇帝', '，', '真', '愛', '相', '<0xE5>', '<0x88>', '<0xA3>', '，', '一生', '常常', 't', 'ī', '戰', '場', '裡', '，', '罕', '得', 't', 'ī', '皇', '宮', '裡', '。', '伊', '的', '對', '敵', '常常', 'kap', '伊', '抵', '<0xE7>', '<0xA1>', '<0x88>', '，', '無', 'h', 'ō', '<0xCD>', '<0x98>', '伊', '一', '時', '仔', '歇', '<0xE7>', '<0x9D>', '<0x8F>', '。', '一個', 'h', 'ō', '<0xCD>', '<0x98>', '伊', '圍', '困', '了', '，', '別', '個', '就', 'k', 'oh', '侵入', '伊', '的', '境界', '。', '所以', '年', '過', '年', '，', '無', '論', '冬天', '，', '無', '論', '熱', '天', '，', '伊', '著', '做', '頭', 'ch', 'h', 'ō', 'a', '伊', '的', '軍', '兵', '，', 'kap', '▁in', '受', '寒', '，', '熱', '，', '<0xE6>', '<0x9E>', '<0xB5>', '餓', '，', '嘴', '乾', '的', '艱', '苦', '。']\n"
     ]
    }
   ],
   "source": [
    "print(\"德國前有一個皇帝，真愛相刣，一生常常tī戰場裡，罕得tī皇宮裡。伊的對敵常常kap伊抵硈，無hō͘伊一時仔歇睏。一個hō͘伊圍困了，別個就koh侵入伊的境界。所以年過年，無論冬天，無論熱天，伊著做頭chhōa伊的軍兵，kap in受寒，熱，枵餓，嘴乾的艱苦。\")\n",
    "print(tokenizer.tokenize(\"德國前有一個皇帝，真愛相刣，一生常常tī戰場裡，罕得tī皇宮裡。伊的對敵常常kap伊抵硈，無hō͘伊一時仔歇睏。一個hō͘伊圍困了，別個就koh侵入伊的境界。所以年過年，無論冬天，無論熱天，伊著做頭chhōa伊的軍兵，kap in受寒，熱，枵餓，嘴乾的艱苦。\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d0d41-fe7d-432f-be7c-c66b5ee90707",
   "metadata": {},
   "source": [
    "# 英文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21f154e7-d123-4da2-b4f7-790294eb3d27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russia’s Investigative Committee announced an investigation into the attack on Telegram, claiming: “Residential and administrative buildings and civilian infrastructure were subjected to mortar and artillery fire. As a result of these criminal actions, several civilians were wounded,”Two areas of the region were then hit overnight by unmanned aerial vehicles (UAVs), according to regional Governor Vyacheslav Gladkov, causing two houses to catch fire.\n",
      "['▁Russia', '’', 's', '▁Investig', 'ative', '▁Committee', '▁announced', '▁an', '▁investigation', '▁into', '▁the', '▁attack', '▁on', '▁Tele', 'gram', ',', '▁claim', 'ing', ':', '▁“', 'Res', 'ident', 'ial', '▁and', '▁administrative', '▁buildings', '▁and', '▁civil', 'ian', '▁infrastr', 'ucture', '▁were', '▁subject', 'ed', '▁to', '▁mort', 'ar', '▁and', '▁art', 'illery', '▁fire', '.', '▁As', '▁a', '▁result', '▁of', '▁these', '▁criminal', '▁actions', ',', '▁several', '▁civ', 'ili', 'ans', '▁were', '▁wounded', ',”', 'Two', '▁areas', '▁of', '▁the', '▁region', '▁were', '▁then', '▁hit', '▁over', 'night', '▁by', '▁un', 'mann', 'ed', '▁aer', 'ial', '▁vehicles', '▁(', 'U', 'AV', 's', '),', '▁according', '▁to', '▁regional', '▁Governor', '▁V', 'y', 'aches', 'lav', '▁Glad', 'kov', ',', '▁causing', '▁two', '▁houses', '▁to', '▁catch', '▁fire', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Russia’s Investigative Committee announced an investigation into the attack on Telegram, claiming: “Residential and administrative buildings and civilian infrastructure were subjected to mortar and artillery fire. As a result of these criminal actions, several civilians were wounded,”Two areas of the region were then hit overnight by unmanned aerial vehicles (UAVs), according to regional Governor Vyacheslav Gladkov, causing two houses to catch fire.\")\n",
    "print(tokenizer.tokenize(\"Russia’s Investigative Committee announced an investigation into the attack on Telegram, claiming: “Residential and administrative buildings and civilian infrastructure were subjected to mortar and artillery fire. As a result of these criminal actions, several civilians were wounded,”Two areas of the region were then hit overnight by unmanned aerial vehicles (UAVs), according to regional Governor Vyacheslav Gladkov, causing two houses to catch fire.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43729d91-beec-408e-861c-f5b2d8a5e5c1",
   "metadata": {},
   "source": [
    "# 繁體字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daf1f78e-1bc0-49f0-b4ee-3d30769c9c88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "台北「拉麵公子」日前推出一款「大王具足蟲」拉麵，不僅將整隻大王具足蟲覆蓋在拉麵上，腺體還可以食用，拉麵公子表示，「口感上比較偏粉嫩鬆散，雖然不及龍蝦與螃蟹扎實的咀嚼感，但吃來解個人生成就也是超級值得」。該餐點曝光後，立刻在網路引起熱議，對此，有網友就好奇發問：「為何能接受龍蝦、螃蟹，不能接受大王具足蟲」，問題一拋出後，隨即掀起熱烈討論。\n",
      "['▁', '台北', '「', '拉', '麵', '公子', '」', '日前', '推出', '一款', '「', '大', '王', '具', '足', '蟲', '」', '拉', '麵', '，', '不', '僅', '將', '整', '隻', '大', '王', '具', '足', '蟲', '覆', '蓋', '在', '拉', '麵', '上', '，', '腺', '體', '還', '可以', '食用', '，', '拉', '麵', '公子', '表示', '，', '「', '口感', '上', '比', '較', '偏', '粉', '嫩', '鬆', '散', '，', '雖然', '不及', '龍', '蝦', '與', '螃', '蟹', '扎', '實', '的', '咀', '嚼', '感', '，', '但', '吃', '來', '解', '個人', '生成', '就', '也是', '超', '級', '值得', '」。', '該', '餐', '點', '曝光', '後', '，', '立刻', '在', '網', '路', '引起', '熱', '議', '，', '對', '此', '，', '有', '網', '友', '就好', '奇', '發', '問', '：', '「', '為', '何', '能', '接受', '龍', '蝦', '、', '螃', '蟹', '，', '不能', '接受', '大', '王', '具', '足', '蟲', '」', '，', '問題', '一', '拋', '出', '後', '，', '隨', '即', '掀起', '熱', '烈', '討', '論', '。']\n"
     ]
    }
   ],
   "source": [
    "print(\"台北「拉麵公子」日前推出一款「大王具足蟲」拉麵，不僅將整隻大王具足蟲覆蓋在拉麵上，腺體還可以食用，拉麵公子表示，「口感上比較偏粉嫩鬆散，雖然不及龍蝦與螃蟹扎實的咀嚼感，但吃來解個人生成就也是超級值得」。該餐點曝光後，立刻在網路引起熱議，對此，有網友就好奇發問：「為何能接受龍蝦、螃蟹，不能接受大王具足蟲」，問題一拋出後，隨即掀起熱烈討論。\")\n",
    "print(tokenizer.tokenize(\"台北「拉麵公子」日前推出一款「大王具足蟲」拉麵，不僅將整隻大王具足蟲覆蓋在拉麵上，腺體還可以食用，拉麵公子表示，「口感上比較偏粉嫩鬆散，雖然不及龍蝦與螃蟹扎實的咀嚼感，但吃來解個人生成就也是超級值得」。該餐點曝光後，立刻在網路引起熱議，對此，有網友就好奇發問：「為何能接受龍蝦、螃蟹，不能接受大王具足蟲」，問題一拋出後，隨即掀起熱烈討論。\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1658e37-95d1-4b58-b00d-c8550d462d49",
   "metadata": {},
   "source": [
    "# 簡體字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "443a3a64-46d5-42c2-8e1e-857cc8808695",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "台北「拉面公子」日前推出一款「大王具足虫」拉面，不仅将整只大王具足虫覆盖在拉面上，腺体还可以食用，拉面公子表示，「口感上比较偏粉嫩松散，虽然不及龙虾与螃蟹扎实的咀嚼感，但吃来解个人生成就也是超级值得」。该餐点曝光后，立刻在网路引起热议，对此，有网友就好奇发问：「为何能接受龙虾、螃蟹，不能接受大王具足虫」，问题一抛出后，随即掀起热烈讨论。\n",
      "['▁', '台北', '「', '拉', '面', '公子', '」', '日前', '推出', '一款', '「', '大', '王', '具', '足', '虫', '」', '拉', '面', '，', '不仅', '将', '整', '只', '大', '王', '具', '足', '虫', '覆盖', '在', '拉', '面上', '，', '腺', '体', '还可以', '食用', '，', '拉', '面', '公子', '表示', '，', '「', '口感', '上', '比较', '偏', '粉', '嫩', '松', '散', '，', '虽然', '不及', '龙', '虾', '与', '螃', '蟹', '扎实', '的', '咀', '嚼', '感', '，', '但', '吃', '来', '解', '个人', '生成', '就', '也是', '超级', '值得', '」。', '该', '餐', '点', '曝光', '后', '，', '立刻', '在', '网', '路', '引起', '热', '议', '，', '对此', '，', '有网友', '就好', '奇', '发', '问', '：', '「', '为何', '能', '接受', '龙', '虾', '、', '螃', '蟹', '，', '不能', '接受', '大', '王', '具', '足', '虫', '」', '，', '问题', '一', '抛', '出', '后', '，', '随即', '掀起', '热烈', '讨论', '。']\n"
     ]
    }
   ],
   "source": [
    "print(\"台北「拉面公子」日前推出一款「大王具足虫」拉面，不仅将整只大王具足虫覆盖在拉面上，腺体还可以食用，拉面公子表示，「口感上比较偏粉嫩松散，虽然不及龙虾与螃蟹扎实的咀嚼感，但吃来解个人生成就也是超级值得」。该餐点曝光后，立刻在网路引起热议，对此，有网友就好奇发问：「为何能接受龙虾、螃蟹，不能接受大王具足虫」，问题一抛出后，随即掀起热烈讨论。\")\n",
    "print(tokenizer.tokenize(\"台北「拉面公子」日前推出一款「大王具足虫」拉面，不仅将整只大王具足虫覆盖在拉面上，腺体还可以食用，拉面公子表示，「口感上比较偏粉嫩松散，虽然不及龙虾与螃蟹扎实的咀嚼感，但吃来解个人生成就也是超级值得」。该餐点曝光后，立刻在网路引起热议，对此，有网友就好奇发问：「为何能接受龙虾、螃蟹，不能接受大王具足虫」，问题一抛出后，随即掀起热烈讨论。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "269085dc-be4b-4ecb-b944-fbc31b797fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从前，有一只蜘蛛，在寺庙的廊庑下静静地结网。由于她经常听到讲经诵法，所以一千年过去了，她变得有些懂得佛理了。有一天，佛从这间寺庙路过，看到了这只有佛缘的蜘蛛，于是问她：“你说，什么是最珍贵的？”蜘蛛想了想说：“是得不到和已失去。”佛笑笑，然后走了。\n",
      "['▁从', '前', '，', '有一', '只', '蜘蛛', '，', '在', '寺庙', '的', '廊', '庑', '下', '静静', '地', '结', '网', '。', '由于', '她', '经常', '听到', '讲', '经', '诵', '法', '，', '所以', '一千', '年', '过去了', '，', '她', '变得', '有些', '懂得', '佛', '理', '了', '。', '有一天', '，', '佛', '从', '这', '间', '寺庙', '路过', '，', '看到了', '这', '只有', '佛', '缘', '的', '蜘蛛', '，', '于是', '问', '她', '：', '“', '你说', '，', '什么是', '最', '珍贵', '的', '？', '”', '蜘蛛', '想', '了', '想', '说', '：', '“', '是', '得不到', '和', '已', '失去', '。”', '佛', '笑', '笑', '，', '然后', '走了', '。']\n"
     ]
    }
   ],
   "source": [
    "print(\"从前，有一只蜘蛛，在寺庙的廊庑下静静地结网。由于她经常听到讲经诵法，所以一千年过去了，她变得有些懂得佛理了。有一天，佛从这间寺庙路过，看到了这只有佛缘的蜘蛛，于是问她：“你说，什么是最珍贵的？”蜘蛛想了想说：“是得不到和已失去。”佛笑笑，然后走了。\")\n",
    "print(tokenizer.tokenize(\"从前，有一只蜘蛛，在寺庙的廊庑下静静地结网。由于她经常听到讲经诵法，所以一千年过去了，她变得有些懂得佛理了。有一天，佛从这间寺庙路过，看到了这只有佛缘的蜘蛛，于是问她：“你说，什么是最珍贵的？”蜘蛛想了想说：“是得不到和已失去。”佛笑笑，然后走了。\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ca92bf-e475-453c-810f-0f55a5102226",
   "metadata": {},
   "source": [
    "# 測試新增字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a4aa4d8-72a7-4639-a269-9241d841e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install protobuf==3.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43252f9b-31d8-4528-aa86-91b6ce278773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n",
    "import sentencepiece as spm\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b829d79-71b4-40a7-9569-f30b593a2ece",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/joeyliang/Chinese-LLaMA-Alpaca/scripts\n",
      "build_dataset.py\n",
      "chinese_sp.model\n",
      "crawl_prompt.py\n",
      "ds_zero2_no_offload.json\n",
      "gradio_demo.py\n",
      "Hokkien.model\n",
      "Hokkien.vocab\n",
      "inference_hf.py\n",
      "\u001b[0m\u001b[01;34mlangchain_demo\u001b[0m/\n",
      "\u001b[01;34mmerged_tokenizer_hf\u001b[0m/\n",
      "\u001b[01;34mmerged_tokenizer_sp\u001b[0m/\n",
      "\u001b[01;34mmerged_traditional_chinese_tokenizer_hf\u001b[0m/\n",
      "\u001b[01;34mmerged_traditional_chinese_tokenizer_sp\u001b[0m/\n",
      "merge_llama_with_chinese_lora.py\n",
      "merge_tokenizers.py\n",
      "run_clm_pt_with_peft.py\n",
      "run_clm_sft_with_peft.py\n",
      "run_pt.sh\n",
      "run_sft.sh\n",
      "tokenizer_checklist\n",
      "tokenizer.model\n",
      "vocab.txt\n",
      "\u001b[01;34m台文資料\u001b[0m/\n",
      "台文資料合併.txt\n"
     ]
    }
   ],
   "source": [
    "%cd scripts\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eaf0ec0-b2ae-4871-adae-b72a044072c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joeyliang/anaconda3/envs/alpaca_lora/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1713: FutureWarning: Calling LlamaTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load\n",
    "llama_tokenizer_dir = \"./tokenizer.model\"\n",
    "chinese_sp_model_file = \"./chinese_sp.model\"\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(llama_tokenizer_dir)\n",
    "chinese_sp_model = spm.SentencePieceProcessor()\n",
    "chinese_sp_model.Load(chinese_sp_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d43c9ff6-dd31-4035-afd9-a281476f3144",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "519964"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_spm = sp_pb2_model.ModelProto()\n",
    "llama_spm.ParseFromString(llama_tokenizer.sp_model.serialized_model_proto())\n",
    "chinese_spm = sp_pb2_model.ModelProto()\n",
    "chinese_spm.ParseFromString(chinese_sp_model.serialized_model_proto())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd38be82-c4ce-4c2b-acaa-060062e2a522",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '人', '工', '智', '能', '是', '计', '算', '机', '科', '学', '、', '心', '理', '学', '、', '<0xE5>', '<0x93>', '<0xB2>', '学', '等', '学', '科', '<0xE8>', '<0x9E>', '<0x8D>', '合', '的', '交', '<0xE5>', '<0x8F>', '<0x89>', '学', '科', '。']\n"
     ]
    }
   ],
   "source": [
    "#origin llama tokenizer\n",
    "print(llama_tokenizer.tokenize(\"人工智能是计算机科学、心理学、哲学等学科融合的交叉学科。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41f2236c-fac7-4f5e-8c35-87f93f336ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 20000\n",
      "['<s>', '</s>', '<unk>']\n",
      "[1, 2, 0]\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n"
     ]
    }
   ],
   "source": [
    "# print number of tokens\n",
    "print(len(llama_tokenizer),len(chinese_sp_model))\n",
    "print(llama_tokenizer.all_special_tokens)\n",
    "print(llama_tokenizer.all_special_ids)\n",
    "print(llama_tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db41c16-dc34-44de-a0a8-c69ca9e9a030",
   "metadata": {},
   "source": [
    "# 建繁體tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72babd41-3475-4a80-96e5-e3d1eed55249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 將簡體字典轉繁體\n",
    "from opencc import OpenCC\n",
    "cc = OpenCC('s2tw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4ba2aeb-df95-4cec-9788-1c1bd54bd924",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_vocabs = [chinese_sp_model.id_to_piece(id) for id in range(chinese_sp_model.get_piece_size())]\n",
    "len(set(origin_vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36d402c8-4656-4a6e-a49c-0e8ca1f9ad54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs = []\n",
    "for id in range(chinese_sp_model.get_piece_size()):\n",
    "    td_chinese = cc.convert(chinese_sp_model.id_to_piece(id)) \n",
    "    if td_chinese in origin_vocabs:\n",
    "        vocabs.append(chinese_sp_model.id_to_piece(id))\n",
    "    else:\n",
    "        vocabs.append(td_chinese)\n",
    "len(set(vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53bbde6b-4736-4326-9280-4962e3f32830",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "Before:32000\n",
      "New model pieces: 49953\n",
      "Chinese-LLaMA tokenizer has been saved to merged_traditional_chinese_tokenizer_hf\n"
     ]
    }
   ],
   "source": [
    "llama_spm_tokens_set=set(p.piece for p in llama_spm.pieces)\n",
    "print(len(llama_spm_tokens_set))\n",
    "print(f\"Before:{len(llama_spm_tokens_set)}\")\n",
    "for p in vocabs:\n",
    "    piece = p\n",
    "    if piece not in llama_spm_tokens_set:\n",
    "        new_p = sp_pb2_model.ModelProto().SentencePiece()\n",
    "        new_p.piece = piece\n",
    "        new_p.score = 0\n",
    "        llama_spm.pieces.append(new_p)\n",
    "print(f\"New model pieces: {len(llama_spm.pieces)}\")\n",
    "\n",
    "## Save\n",
    "trad_output_sp_dir = 'merged_traditional_chinese_tokenizer_sp'\n",
    "trad_output_hf_dir = 'merged_traditional_chinese_tokenizer_hf' # the path to save Chinese-LLaMA tokenizer\n",
    "os.makedirs(trad_output_sp_dir,exist_ok=True)\n",
    "with open(trad_output_sp_dir+'/tradictional_chinese_llama.model', 'wb') as f:\n",
    "    f.write(llama_spm.SerializeToString())\n",
    "tokenizer = LlamaTokenizer(vocab_file=trad_output_sp_dir+'/tradictional_chinese_llama.model')\n",
    "\n",
    "tokenizer.save_pretrained(trad_output_hf_dir)\n",
    "print(f\"Chinese-LLaMA tokenizer has been saved to {trad_output_hf_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeac611-e5c2-41db-8a95-ec964eb0dd38",
   "metadata": {},
   "source": [
    "# 建簡體tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "921758ad-ad9e-4f7b-8f88-9d6f95d3739a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "Before:32000\n",
      "New model pieces: 49953\n",
      "Chinese-LLaMA tokenizer has been saved to merged_tokenizer_hf\n"
     ]
    }
   ],
   "source": [
    "llama_spm = sp_pb2_model.ModelProto()\n",
    "llama_spm.ParseFromString(llama_tokenizer.sp_model.serialized_model_proto())\n",
    "chinese_spm = sp_pb2_model.ModelProto()\n",
    "chinese_spm.ParseFromString(chinese_sp_model.serialized_model_proto())\n",
    "## Add Chinese tokens to LLaMA tokenizer\n",
    "llama_spm_tokens_set=set(p.piece for p in llama_spm.pieces)\n",
    "print(len(llama_spm_tokens_set))\n",
    "print(f\"Before:{len(llama_spm_tokens_set)}\")\n",
    "for p in chinese_spm.pieces:\n",
    "    piece = p.piece\n",
    "    if piece not in llama_spm_tokens_set:\n",
    "        new_p = sp_pb2_model.ModelProto().SentencePiece()\n",
    "        new_p.piece = piece\n",
    "        new_p.score = 0\n",
    "        llama_spm.pieces.append(new_p)\n",
    "print(f\"New model pieces: {len(llama_spm.pieces)}\")\n",
    "\n",
    "## Save\n",
    "output_sp_dir = 'merged_tokenizer_sp'\n",
    "output_hf_dir = 'merged_tokenizer_hf' # the path to save Chinese-LLaMA tokenizer\n",
    "os.makedirs(output_sp_dir,exist_ok=True)\n",
    "with open(output_sp_dir+'/chinese_llama.model', 'wb') as f:\n",
    "    f.write(llama_spm.SerializeToString())\n",
    "tokenizer = LlamaTokenizer(vocab_file=output_sp_dir+'/chinese_llama.model')\n",
    "\n",
    "tokenizer.save_pretrained(output_hf_dir)\n",
    "print(f\"Chinese-LLaMA tokenizer has been saved to {output_hf_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0be27f7d-937a-4335-9599-e67b99b9a875",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '</s>', '<unk>']\n",
      "[1, 2, 0]\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "Test text:\n",
      " 人工智能是计算机科学、心理学、哲学等学科融合的交叉学科。\n",
      "Tokenized by LLaMA tokenizer:\n",
      "['▁', '人', '工', '智', '能', '是', '计', '算', '机', '科', '学', '、', '心', '理', '学', '、', '<0xE5>', '<0x93>', '<0xB2>', '学', '等', '学', '科', '<0xE8>', '<0x9E>', '<0x8D>', '合', '的', '交', '<0xE5>', '<0x8F>', '<0x89>', '学', '科', '。']\n",
      "\n",
      "Tokenized by Chinese-LLaMA tokenizer:\n",
      "['▁', '人工智能', '是', '计算机', '科学', '、', '心理学', '、', '哲学', '等', '学科', '融合', '的', '交叉', '学科', '。']\n",
      "\n",
      "Tokenized by tradictional Chinese-LLaMA tokenizer:\n",
      "['▁', '人工智能', '是', '計算機', '科學', '、', '心理學', '、', '哲學', '等', '學科', '融合', '的', '交叉', '學科', '。']\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(llama_tokenizer_dir)\n",
    "chinese_llama_tokenizer = LlamaTokenizer.from_pretrained(output_hf_dir)\n",
    "tradictional_chinese_llama_tokenizer = LlamaTokenizer.from_pretrained(trad_output_hf_dir)\n",
    "print(tokenizer.all_special_tokens)\n",
    "print(tokenizer.all_special_ids)\n",
    "print(tokenizer.special_tokens_map)\n",
    "text = \"人工智能是计算机科学、心理学、哲学等学科融合的交叉学科。\"\n",
    "text2 = cc.convert(text)\n",
    "print(\"Test text:\\n\",text)\n",
    "print(f\"Tokenized by LLaMA tokenizer:\\n{llama_tokenizer.tokenize(text)}\")\n",
    "print()\n",
    "print(f\"Tokenized by Chinese-LLaMA tokenizer:\\n{chinese_llama_tokenizer.tokenize(text)}\")\n",
    "print()\n",
    "print(f\"Tokenized by tradictional Chinese-LLaMA tokenizer:\\n{tradictional_chinese_llama_tokenizer.tokenize(text2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f58c400-4a5c-42ec-82c5-766a0b0e5c18",
   "metadata": {},
   "source": [
    "# train sentencepiece model\n",
    "- input: one-sentence-per-line raw corpus file. No need to run tokenizer, normalizer or preprocessor. By default, SentencePiece normalizes the input with Unicode NFKC. You can pass a comma-separated list of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3cd4da7-e944-4724-8f14-da2304a32787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "hokkien_files = glob.glob(\"台文資料/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e2bca80-85c5-47d7-a331-4c12f27ea366",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['台文資料/臺語國校仔課本.txt',\n",
       " '台文資料/閱讀越懂閩客語.txt',\n",
       " '台文資料/台灣新眼界.txt',\n",
       " '台文資料/tw_lyrics.txt',\n",
       " '台文資料/台灣新眼界_gpt_加標點分段落.txt',\n",
       " '台文資料/寶島鼓仔燈.txt',\n",
       " '台文資料/台灣記事簿_gpt_加標點分段落.txt',\n",
       " '台文資料/TAT.txt',\n",
       " '台文資料/moedict_norm.txt',\n",
       " '台文資料/台灣記事簿.txt',\n",
       " '台文資料/寶島鼓仔燈_gpt_加標點分段落.txt',\n",
       " '台文資料/taigilongthok_all.txt']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hokkien_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6d9e600-2460-4cb1-afab-b4bed4c20a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63477\n"
     ]
    }
   ],
   "source": [
    "all_txt = []\n",
    "for files in hokkien_files:\n",
    "    f = open(files, 'r').read().split(\"\\n\\n\")\n",
    "    all_txt+=f\n",
    "print(len(all_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7a634a8-3fe8-4e95-bed2-b11e22b87d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_txt_copy = all_txt.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8326839-6163-4f8d-99f6-e8d32a7c877c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_txt_preprocessed = []\n",
    "for txt in all_txt:\n",
    "    txt = txt.split(\"\\n\")\n",
    "    tmp = \"\"\n",
    "    for sen in txt:\n",
    "        if sen.endswith(\"，\") or sen.endswith(\"。\") or sen.endswith(\"？\") or sen.endswith(\"?\") or sen.endswith(\"!\") or sen.endswith(\"！\")or  sen.endswith(\"」\"):\n",
    "            tmp = tmp + sen\n",
    "        else:\n",
    "            if sen == txt[-1]:\n",
    "                tmp = tmp + sen+\"。\"\n",
    "            else:\n",
    "                tmp = tmp + sen+\"，\"\n",
    "    all_txt_preprocessed.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be3312e5-04a2-425b-8cec-adeaee5a7e76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'寶島的小姑娘，十八當青春。滿街路，青紅燈閃爍，阮怎偏偏心稀微，阮雖然踮在這位，已經過三年，猶原也毋願袂記心愛故鄉的滋味，莎喲娜啦，莎喲娜啦，繁華的都市生活，莎喲娜啦。'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_txt_preprocessed[39232]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf446b9c-bfc4-45b2-886b-6a9604492bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"台文資料合併.txt\",\"w\") as f:\n",
    "    for text in all_txt_preprocessed:\n",
    "        f.write(text+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2e5216c-3057-49e2-bcd7-7f56134a8bde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: 白話字數位典藏館(白話字).txt\n",
      "  input_format: \n",
      "  model_prefix: 白話字測試版\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 3996\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: 白話字數位典藏館(白話字).txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 5346 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=1556579\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9504% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=90\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999504\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 5346 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=763814\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 5728 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 5346\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 18381\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 18381 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4578 obj=11.7986 num_tokens=57372 num_tokens/piece=12.5321\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3972 obj=9.78153 num_tokens=57440 num_tokens/piece=14.4612\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: 白話字測試版.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: 白話字測試版.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(input='白話字數位典藏館(白話字).txt', \n",
    "                               model_prefix='白話字測試版',\n",
    "                               vocab_size=3996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7488c433-ea4b-4931-8984-d6eec35678ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
